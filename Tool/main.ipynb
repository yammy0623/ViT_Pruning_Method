{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christine_Hsieh\\AppData\\Local\\Continuum\\anaconda3\\envs\\tome\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import tome\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model_name = \"vit_base_patch16_224\"\n",
    "model = timm.create_model(model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setting\n",
    "device = \"cuda:0\"\n",
    "runs = 50\n",
    "batch_size = 256  # Lower this if you don't have that much memory\n",
    "input_size = model.default_cfg[\"input_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\Christine_Hsieh\\AppData\\Local\\Continuum\\anaconda3\\envs\\tome\\lib\\site-packages\\timm\\models\\vision_transformer.py:92: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  x = F.scaled_dot_product_attention(\n",
      "Benchmarking: 100%|██████████| 50/50 [01:13<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 149.04 im/s\n"
     ]
    }
   ],
   "source": [
    "# Baseline benchmark\n",
    "baseline_throughput = tome.utils.benchmark(\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    runs=runs,\n",
    "    batch_size=batch_size,\n",
    "    input_size=input_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VisionTransformer                        [256, 1000]               152,064\n",
       "├─PatchEmbed: 1-1                        [256, 196, 768]           --\n",
       "│    └─Conv2d: 2-1                       [256, 768, 14, 14]        590,592\n",
       "│    └─Identity: 2-2                     [256, 196, 768]           --\n",
       "├─Dropout: 1-2                           [256, 197, 768]           --\n",
       "├─Identity: 1-3                          [256, 197, 768]           --\n",
       "├─Identity: 1-4                          [256, 197, 768]           --\n",
       "├─Sequential: 1-5                        [256, 197, 768]           --\n",
       "│    └─Block: 2-3                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-1               [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-2               [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-3                [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-4                [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-5               [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-6                     [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-7                [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-8                [256, 197, 768]           --\n",
       "│    └─Block: 2-4                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-9               [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-10              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-11               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-12               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-13              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-14                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-15               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-16               [256, 197, 768]           --\n",
       "│    └─Block: 2-5                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-17              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-18              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-19               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-20               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-21              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-22                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-23               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-24               [256, 197, 768]           --\n",
       "│    └─Block: 2-6                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-25              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-26              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-27               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-28               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-29              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-30                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-31               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-32               [256, 197, 768]           --\n",
       "│    └─Block: 2-7                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-33              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-34              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-35               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-36               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-37              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-38                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-39               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-40               [256, 197, 768]           --\n",
       "│    └─Block: 2-8                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-41              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-42              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-43               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-44               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-45              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-46                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-47               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-48               [256, 197, 768]           --\n",
       "│    └─Block: 2-9                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-49              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-50              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-51               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-52               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-53              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-54                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-55               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-56               [256, 197, 768]           --\n",
       "│    └─Block: 2-10                       [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-57              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-58              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-59               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-60               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-61              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-62                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-63               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-64               [256, 197, 768]           --\n",
       "│    └─Block: 2-11                       [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-65              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-66              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-67               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-68               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-69              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-70                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-71               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-72               [256, 197, 768]           --\n",
       "│    └─Block: 2-12                       [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-73              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-74              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-75               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-76               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-77              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-78                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-79               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-80               [256, 197, 768]           --\n",
       "│    └─Block: 2-13                       [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-81              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-82              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-83               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-84               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-85              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-86                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-87               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-88               [256, 197, 768]           --\n",
       "│    └─Block: 2-14                       [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-89              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-90              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-91               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-92               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-93              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-94                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-95               [256, 197, 768]           --\n",
       "│    │    └─Identity: 3-96               [256, 197, 768]           --\n",
       "├─LayerNorm: 1-6                         [256, 197, 768]           1,536\n",
       "├─Identity: 1-7                          [256, 768]                --\n",
       "├─Dropout: 1-8                           [256, 768]                --\n",
       "├─Linear: 1-9                            [256, 1000]               769,000\n",
       "==========================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 51.60\n",
       "==========================================================================================\n",
       "Input size (MB): 154.14\n",
       "Forward/backward pass size (MB): 41520.94\n",
       "Params size (MB): 345.66\n",
       "Estimated Total Size (MB): 42020.74\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Apply ToMe \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|██████████| 50/50 [00:51<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 247.85 im/s\n",
      "Throughput improvement: 1.66x\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Apply ToMe \\n\")\n",
    "tome.patch.timm(model)\n",
    "# ToMe with r=16\n",
    "model.r = 16\n",
    "tome_throughput = tome.utils.benchmark(\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    runs=runs,\n",
    "    batch_size=batch_size,\n",
    "    input_size=input_size\n",
    ")\n",
    "print(f\"Throughput improvement: {tome_throughput / baseline_throughput:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ToMeVisionTransformer                    [256, 1000]               152,064\n",
       "├─PatchEmbed: 1-1                        [256, 196, 768]           --\n",
       "│    └─Conv2d: 2-1                       [256, 768, 14, 14]        590,592\n",
       "│    └─Identity: 2-2                     [256, 196, 768]           --\n",
       "├─Dropout: 1-2                           [256, 197, 768]           --\n",
       "├─Identity: 1-3                          [256, 197, 768]           --\n",
       "├─Identity: 1-4                          [256, 197, 768]           --\n",
       "├─Sequential: 1-5                        [256, 11, 768]            --\n",
       "│    └─ToMeBlock: 2-3                    [256, 181, 768]           --\n",
       "│    │    └─LayerNorm: 3-1               [256, 197, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-2           [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-3                [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-4               [256, 181, 768]           1,536\n",
       "│    │    └─Mlp: 3-5                     [256, 181, 768]           4,722,432\n",
       "│    │    └─Identity: 3-6                [256, 181, 768]           --\n",
       "│    └─ToMeBlock: 2-4                    [256, 165, 768]           --\n",
       "│    │    └─LayerNorm: 3-7               [256, 181, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-8           [256, 181, 768]           2,362,368\n",
       "│    │    └─Identity: 3-9                [256, 181, 768]           --\n",
       "│    │    └─LayerNorm: 3-10              [256, 165, 768]           1,536\n",
       "│    │    └─Mlp: 3-11                    [256, 165, 768]           4,722,432\n",
       "│    │    └─Identity: 3-12               [256, 165, 768]           --\n",
       "│    └─ToMeBlock: 2-5                    [256, 149, 768]           --\n",
       "│    │    └─LayerNorm: 3-13              [256, 165, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-14          [256, 165, 768]           2,362,368\n",
       "│    │    └─Identity: 3-15               [256, 165, 768]           --\n",
       "│    │    └─LayerNorm: 3-16              [256, 149, 768]           1,536\n",
       "│    │    └─Mlp: 3-17                    [256, 149, 768]           4,722,432\n",
       "│    │    └─Identity: 3-18               [256, 149, 768]           --\n",
       "│    └─ToMeBlock: 2-6                    [256, 133, 768]           --\n",
       "│    │    └─LayerNorm: 3-19              [256, 149, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-20          [256, 149, 768]           2,362,368\n",
       "│    │    └─Identity: 3-21               [256, 149, 768]           --\n",
       "│    │    └─LayerNorm: 3-22              [256, 133, 768]           1,536\n",
       "│    │    └─Mlp: 3-23                    [256, 133, 768]           4,722,432\n",
       "│    │    └─Identity: 3-24               [256, 133, 768]           --\n",
       "│    └─ToMeBlock: 2-7                    [256, 117, 768]           --\n",
       "│    │    └─LayerNorm: 3-25              [256, 133, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-26          [256, 133, 768]           2,362,368\n",
       "│    │    └─Identity: 3-27               [256, 133, 768]           --\n",
       "│    │    └─LayerNorm: 3-28              [256, 117, 768]           1,536\n",
       "│    │    └─Mlp: 3-29                    [256, 117, 768]           4,722,432\n",
       "│    │    └─Identity: 3-30               [256, 117, 768]           --\n",
       "│    └─ToMeBlock: 2-8                    [256, 101, 768]           --\n",
       "│    │    └─LayerNorm: 3-31              [256, 117, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-32          [256, 117, 768]           2,362,368\n",
       "│    │    └─Identity: 3-33               [256, 117, 768]           --\n",
       "│    │    └─LayerNorm: 3-34              [256, 101, 768]           1,536\n",
       "│    │    └─Mlp: 3-35                    [256, 101, 768]           4,722,432\n",
       "│    │    └─Identity: 3-36               [256, 101, 768]           --\n",
       "│    └─ToMeBlock: 2-9                    [256, 85, 768]            --\n",
       "│    │    └─LayerNorm: 3-37              [256, 101, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-38          [256, 101, 768]           2,362,368\n",
       "│    │    └─Identity: 3-39               [256, 101, 768]           --\n",
       "│    │    └─LayerNorm: 3-40              [256, 85, 768]            1,536\n",
       "│    │    └─Mlp: 3-41                    [256, 85, 768]            4,722,432\n",
       "│    │    └─Identity: 3-42               [256, 85, 768]            --\n",
       "│    └─ToMeBlock: 2-10                   [256, 69, 768]            --\n",
       "│    │    └─LayerNorm: 3-43              [256, 85, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-44          [256, 85, 768]            2,362,368\n",
       "│    │    └─Identity: 3-45               [256, 85, 768]            --\n",
       "│    │    └─LayerNorm: 3-46              [256, 69, 768]            1,536\n",
       "│    │    └─Mlp: 3-47                    [256, 69, 768]            4,722,432\n",
       "│    │    └─Identity: 3-48               [256, 69, 768]            --\n",
       "│    └─ToMeBlock: 2-11                   [256, 53, 768]            --\n",
       "│    │    └─LayerNorm: 3-49              [256, 69, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-50          [256, 69, 768]            2,362,368\n",
       "│    │    └─Identity: 3-51               [256, 69, 768]            --\n",
       "│    │    └─LayerNorm: 3-52              [256, 53, 768]            1,536\n",
       "│    │    └─Mlp: 3-53                    [256, 53, 768]            4,722,432\n",
       "│    │    └─Identity: 3-54               [256, 53, 768]            --\n",
       "│    └─ToMeBlock: 2-12                   [256, 37, 768]            --\n",
       "│    │    └─LayerNorm: 3-55              [256, 53, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-56          [256, 53, 768]            2,362,368\n",
       "│    │    └─Identity: 3-57               [256, 53, 768]            --\n",
       "│    │    └─LayerNorm: 3-58              [256, 37, 768]            1,536\n",
       "│    │    └─Mlp: 3-59                    [256, 37, 768]            4,722,432\n",
       "│    │    └─Identity: 3-60               [256, 37, 768]            --\n",
       "│    └─ToMeBlock: 2-13                   [256, 21, 768]            --\n",
       "│    │    └─LayerNorm: 3-61              [256, 37, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-62          [256, 37, 768]            2,362,368\n",
       "│    │    └─Identity: 3-63               [256, 37, 768]            --\n",
       "│    │    └─LayerNorm: 3-64              [256, 21, 768]            1,536\n",
       "│    │    └─Mlp: 3-65                    [256, 21, 768]            4,722,432\n",
       "│    │    └─Identity: 3-66               [256, 21, 768]            --\n",
       "│    └─ToMeBlock: 2-14                   [256, 11, 768]            --\n",
       "│    │    └─LayerNorm: 3-67              [256, 21, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-68          [256, 21, 768]            2,362,368\n",
       "│    │    └─Identity: 3-69               [256, 21, 768]            --\n",
       "│    │    └─LayerNorm: 3-70              [256, 11, 768]            1,536\n",
       "│    │    └─Mlp: 3-71                    [256, 11, 768]            4,722,432\n",
       "│    │    └─Identity: 3-72               [256, 11, 768]            --\n",
       "├─LayerNorm: 1-6                         [256, 11, 768]            1,536\n",
       "├─Identity: 1-7                          [256, 768]                --\n",
       "├─Dropout: 1-8                           [256, 768]                --\n",
       "├─Linear: 1-9                            [256, 1000]               769,000\n",
       "==========================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 51.60\n",
       "==========================================================================================\n",
       "Input size (MB): 154.14\n",
       "Forward/backward pass size (MB): 21202.68\n",
       "Params size (MB): 345.66\n",
       "Estimated Total Size (MB): 21702.48\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Apply ToMe with decreasing schedule \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|██████████| 50/50 [00:35<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 364.03 im/s\n",
      "Throughput improvement: 2.44x\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Apply ToMe with decreasing schedule \\n\")\n",
    "model.r = (16, -1.0)\n",
    "tome_decr_throughput = tome.utils.benchmark(\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    runs=runs,\n",
    "    batch_size=batch_size,\n",
    "    input_size=input_size\n",
    ")\n",
    "print(f\"Throughput improvement: {tome_decr_throughput / baseline_throughput:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ToMeVisionTransformer                    [256, 1000]               152,064\n",
       "├─PatchEmbed: 1-1                        [256, 196, 768]           --\n",
       "│    └─Conv2d: 2-1                       [256, 768, 14, 14]        590,592\n",
       "│    └─Identity: 2-2                     [256, 196, 768]           --\n",
       "├─Dropout: 1-2                           [256, 197, 768]           --\n",
       "├─Identity: 1-3                          [256, 197, 768]           --\n",
       "├─Identity: 1-4                          [256, 197, 768]           --\n",
       "├─Sequential: 1-5                        [256, 10, 768]            --\n",
       "│    └─ToMeBlock: 2-3                    [256, 165, 768]           --\n",
       "│    │    └─LayerNorm: 3-1               [256, 197, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-2           [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-3                [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-4               [256, 165, 768]           1,536\n",
       "│    │    └─Mlp: 3-5                     [256, 165, 768]           4,722,432\n",
       "│    │    └─Identity: 3-6                [256, 165, 768]           --\n",
       "│    └─ToMeBlock: 2-4                    [256, 136, 768]           --\n",
       "│    │    └─LayerNorm: 3-7               [256, 165, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-8           [256, 165, 768]           2,362,368\n",
       "│    │    └─Identity: 3-9                [256, 165, 768]           --\n",
       "│    │    └─LayerNorm: 3-10              [256, 136, 768]           1,536\n",
       "│    │    └─Mlp: 3-11                    [256, 136, 768]           4,722,432\n",
       "│    │    └─Identity: 3-12               [256, 136, 768]           --\n",
       "│    └─ToMeBlock: 2-5                    [256, 110, 768]           --\n",
       "│    │    └─LayerNorm: 3-13              [256, 136, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-14          [256, 136, 768]           2,362,368\n",
       "│    │    └─Identity: 3-15               [256, 136, 768]           --\n",
       "│    │    └─LayerNorm: 3-16              [256, 110, 768]           1,536\n",
       "│    │    └─Mlp: 3-17                    [256, 110, 768]           4,722,432\n",
       "│    │    └─Identity: 3-18               [256, 110, 768]           --\n",
       "│    └─ToMeBlock: 2-6                    [256, 87, 768]            --\n",
       "│    │    └─LayerNorm: 3-19              [256, 110, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-20          [256, 110, 768]           2,362,368\n",
       "│    │    └─Identity: 3-21               [256, 110, 768]           --\n",
       "│    │    └─LayerNorm: 3-22              [256, 87, 768]            1,536\n",
       "│    │    └─Mlp: 3-23                    [256, 87, 768]            4,722,432\n",
       "│    │    └─Identity: 3-24               [256, 87, 768]            --\n",
       "│    └─ToMeBlock: 2-7                    [256, 67, 768]            --\n",
       "│    │    └─LayerNorm: 3-25              [256, 87, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-26          [256, 87, 768]            2,362,368\n",
       "│    │    └─Identity: 3-27               [256, 87, 768]            --\n",
       "│    │    └─LayerNorm: 3-28              [256, 67, 768]            1,536\n",
       "│    │    └─Mlp: 3-29                    [256, 67, 768]            4,722,432\n",
       "│    │    └─Identity: 3-30               [256, 67, 768]            --\n",
       "│    └─ToMeBlock: 2-8                    [256, 50, 768]            --\n",
       "│    │    └─LayerNorm: 3-31              [256, 67, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-32          [256, 67, 768]            2,362,368\n",
       "│    │    └─Identity: 3-33               [256, 67, 768]            --\n",
       "│    │    └─LayerNorm: 3-34              [256, 50, 768]            1,536\n",
       "│    │    └─Mlp: 3-35                    [256, 50, 768]            4,722,432\n",
       "│    │    └─Identity: 3-36               [256, 50, 768]            --\n",
       "│    └─ToMeBlock: 2-9                    [256, 36, 768]            --\n",
       "│    │    └─LayerNorm: 3-37              [256, 50, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-38          [256, 50, 768]            2,362,368\n",
       "│    │    └─Identity: 3-39               [256, 50, 768]            --\n",
       "│    │    └─LayerNorm: 3-40              [256, 36, 768]            1,536\n",
       "│    │    └─Mlp: 3-41                    [256, 36, 768]            4,722,432\n",
       "│    │    └─Identity: 3-42               [256, 36, 768]            --\n",
       "│    └─ToMeBlock: 2-10                   [256, 25, 768]            --\n",
       "│    │    └─LayerNorm: 3-43              [256, 36, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-44          [256, 36, 768]            2,362,368\n",
       "│    │    └─Identity: 3-45               [256, 36, 768]            --\n",
       "│    │    └─LayerNorm: 3-46              [256, 25, 768]            1,536\n",
       "│    │    └─Mlp: 3-47                    [256, 25, 768]            4,722,432\n",
       "│    │    └─Identity: 3-48               [256, 25, 768]            --\n",
       "│    └─ToMeBlock: 2-11                   [256, 17, 768]            --\n",
       "│    │    └─LayerNorm: 3-49              [256, 25, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-50          [256, 25, 768]            2,362,368\n",
       "│    │    └─Identity: 3-51               [256, 25, 768]            --\n",
       "│    │    └─LayerNorm: 3-52              [256, 17, 768]            1,536\n",
       "│    │    └─Mlp: 3-53                    [256, 17, 768]            4,722,432\n",
       "│    │    └─Identity: 3-54               [256, 17, 768]            --\n",
       "│    └─ToMeBlock: 2-12                   [256, 12, 768]            --\n",
       "│    │    └─LayerNorm: 3-55              [256, 17, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-56          [256, 17, 768]            2,362,368\n",
       "│    │    └─Identity: 3-57               [256, 17, 768]            --\n",
       "│    │    └─LayerNorm: 3-58              [256, 12, 768]            1,536\n",
       "│    │    └─Mlp: 3-59                    [256, 12, 768]            4,722,432\n",
       "│    │    └─Identity: 3-60               [256, 12, 768]            --\n",
       "│    └─ToMeBlock: 2-13                   [256, 10, 768]            --\n",
       "│    │    └─LayerNorm: 3-61              [256, 12, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-62          [256, 12, 768]            2,362,368\n",
       "│    │    └─Identity: 3-63               [256, 12, 768]            --\n",
       "│    │    └─LayerNorm: 3-64              [256, 10, 768]            1,536\n",
       "│    │    └─Mlp: 3-65                    [256, 10, 768]            4,722,432\n",
       "│    │    └─Identity: 3-66               [256, 10, 768]            --\n",
       "│    └─ToMeBlock: 2-14                   [256, 10, 768]            --\n",
       "│    │    └─LayerNorm: 3-67              [256, 10, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-68          [256, 10, 768]            2,362,368\n",
       "│    │    └─Identity: 3-69               [256, 10, 768]            --\n",
       "│    │    └─LayerNorm: 3-70              [256, 10, 768]            1,536\n",
       "│    │    └─Mlp: 3-71                    [256, 10, 768]            4,722,432\n",
       "│    │    └─Identity: 3-72               [256, 10, 768]            --\n",
       "├─LayerNorm: 1-6                         [256, 10, 768]            1,536\n",
       "├─Identity: 1-7                          [256, 768]                --\n",
       "├─Dropout: 1-8                           [256, 768]                --\n",
       "├─Linear: 1-9                            [256, 1000]               769,000\n",
       "==========================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 51.60\n",
       "==========================================================================================\n",
       "Input size (MB): 154.14\n",
       "Forward/backward pass size (MB): 14340.28\n",
       "Params size (MB): 345.66\n",
       "Estimated Total Size (MB): 14840.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_mobilenetv3_large_075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|██████████| 50/50 [00:08<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 1418.41 im/s\n"
     ]
    }
   ],
   "source": [
    "# Load mobileNet\n",
    "model_name = \"tf_mobilenetv3_large_075\"\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "device = \"cuda:0\"\n",
    "runs = 50\n",
    "batch_size = 256  # Lower this if you don't have that much memory\n",
    "input_size = model.default_cfg[\"input_size\"]\n",
    "print(model_name)\n",
    "baseline_throughput = tome.utils.benchmark(\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    runs=runs,\n",
    "    batch_size=batch_size,\n",
    "    input_size=input_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "MobileNetV3                                   [256, 1000]               --\n",
       "├─Conv2dSame: 1-1                             [256, 16, 112, 112]       432\n",
       "├─BatchNormAct2d: 1-2                         [256, 16, 112, 112]       32\n",
       "│    └─Identity: 2-1                          [256, 16, 112, 112]       --\n",
       "│    └─Hardswish: 2-2                         [256, 16, 112, 112]       --\n",
       "├─Sequential: 1-3                             [256, 720, 7, 7]          --\n",
       "│    └─Sequential: 2-3                        [256, 16, 112, 112]       --\n",
       "│    │    └─DepthwiseSeparableConv: 3-1       [256, 16, 112, 112]       464\n",
       "│    └─Sequential: 2-4                        [256, 24, 56, 56]         --\n",
       "│    │    └─InvertedResidual: 3-2             [256, 24, 56, 56]         3,440\n",
       "│    │    └─InvertedResidual: 3-3             [256, 24, 56, 56]         4,440\n",
       "│    └─Sequential: 2-5                        [256, 32, 28, 28]         --\n",
       "│    │    └─InvertedResidual: 3-4             [256, 32, 28, 28]         9,736\n",
       "│    │    └─InvertedResidual: 3-5             [256, 32, 28, 28]         13,720\n",
       "│    │    └─InvertedResidual: 3-6             [256, 32, 28, 28]         13,720\n",
       "│    └─Sequential: 2-6                        [256, 64, 14, 14]         --\n",
       "│    │    └─InvertedResidual: 3-7             [256, 64, 14, 14]         21,056\n",
       "│    │    └─InvertedResidual: 3-8             [256, 64, 14, 14]         22,688\n",
       "│    │    └─InvertedResidual: 3-9             [256, 64, 14, 14]         20,432\n",
       "│    │    └─InvertedResidual: 3-10            [256, 64, 14, 14]         20,432\n",
       "│    └─Sequential: 2-7                        [256, 88, 14, 14]         --\n",
       "│    │    └─InvertedResidual: 3-11            [256, 88, 14, 14]         137,744\n",
       "│    │    └─InvertedResidual: 3-12            [256, 88, 14, 14]         244,248\n",
       "│    └─Sequential: 2-8                        [256, 120, 7, 7]          --\n",
       "│    │    └─InvertedResidual: 3-13            [256, 120, 7, 7]          269,656\n",
       "│    │    └─InvertedResidual: 3-14            [256, 120, 7, 7]          459,784\n",
       "│    │    └─InvertedResidual: 3-15            [256, 120, 7, 7]          459,784\n",
       "│    └─Sequential: 2-9                        [256, 720, 7, 7]          --\n",
       "│    │    └─ConvBnAct: 3-16                   [256, 720, 7, 7]          87,840\n",
       "├─SelectAdaptivePool2d: 1-4                   [256, 720, 1, 1]          --\n",
       "│    └─AdaptiveAvgPool2d: 2-10                [256, 720, 1, 1]          --\n",
       "│    └─Identity: 2-11                         [256, 720, 1, 1]          --\n",
       "├─Conv2d: 1-5                                 [256, 1280, 1, 1]         922,880\n",
       "├─Identity: 1-6                               [256, 1280, 1, 1]         --\n",
       "├─Hardswish: 1-7                              [256, 1280, 1, 1]         --\n",
       "├─Flatten: 1-8                                [256, 1280]               --\n",
       "├─Linear: 1-9                                 [256, 1000]               1,281,000\n",
       "===============================================================================================\n",
       "Total params: 3,993,528\n",
       "Trainable params: 3,993,528\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 39.57\n",
       "===============================================================================================\n",
       "Input size (MB): 154.14\n",
       "Forward/backward pass size (MB): 8195.87\n",
       "Params size (MB): 15.90\n",
       "Estimated Total Size (MB): 8365.91\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenetv4_hybrid_medium_075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|██████████| 50/50 [00:10<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 1206.70 im/s\n"
     ]
    }
   ],
   "source": [
    "# Load mobileNet\n",
    "model_name = \"mobilenetv4_hybrid_medium_075\"\n",
    "model = timm.create_model(model_name, pretrained=False)\n",
    "device = \"cuda:0\"\n",
    "runs = 50\n",
    "batch_size = 256  # Lower this if you don't have that much memory\n",
    "input_size = model.default_cfg[\"input_size\"]\n",
    "print(model_name)\n",
    "baseline_throughput = tome.utils.benchmark(\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    runs=runs,\n",
    "    batch_size=batch_size,\n",
    "    input_size=input_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "MobileNetV3                                        [256, 1000]               --\n",
       "├─Conv2d: 1-1                                      [256, 32, 112, 112]       864\n",
       "├─BatchNormAct2d: 1-2                              [256, 32, 112, 112]       64\n",
       "│    └─Identity: 2-1                               [256, 32, 112, 112]       --\n",
       "│    └─ReLU: 2-2                                   [256, 32, 112, 112]       --\n",
       "├─Sequential: 1-3                                  [256, 720, 7, 7]          --\n",
       "│    └─Sequential: 2-3                             [256, 40, 56, 56]         --\n",
       "│    │    └─EdgeResidual: 3-1                      [256, 40, 56, 56]         42,320\n",
       "│    └─Sequential: 2-4                             [256, 64, 28, 28]         --\n",
       "│    │    └─UniversalInvertedResidual: 3-2         [256, 64, 28, 28]         21,912\n",
       "│    │    └─UniversalInvertedResidual: 3-3         [256, 64, 28, 28]         18,944\n",
       "│    └─Sequential: 2-5                             [256, 120, 14, 14]        --\n",
       "│    │    └─UniversalInvertedResidual: 3-4         [256, 120, 14, 14]        82,856\n",
       "│    │    └─UniversalInvertedResidual: 3-5         [256, 120, 14, 14]        58,440\n",
       "│    │    └─UniversalInvertedResidual: 3-6         [256, 120, 14, 14]        123,120\n",
       "│    │    └─UniversalInvertedResidual: 3-7         [256, 120, 14, 14]        130,800\n",
       "│    │    └─MobileAttention: 3-8                   [256, 120, 14, 14]        79,800\n",
       "│    │    └─UniversalInvertedResidual: 3-9         [256, 120, 14, 14]        123,120\n",
       "│    │    └─MobileAttention: 3-10                  [256, 120, 14, 14]        79,800\n",
       "│    │    └─UniversalInvertedResidual: 3-11        [256, 120, 14, 14]        117,840\n",
       "│    │    └─MobileAttention: 3-12                  [256, 120, 14, 14]        79,800\n",
       "│    │    └─UniversalInvertedResidual: 3-13        [256, 120, 14, 14]        123,120\n",
       "│    │    └─MobileAttention: 3-14                  [256, 120, 14, 14]        79,800\n",
       "│    │    └─UniversalInvertedResidual: 3-15        [256, 120, 14, 14]        117,840\n",
       "│    └─Sequential: 2-6                             [256, 192, 7, 7]          --\n",
       "│    │    └─UniversalInvertedResidual: 3-16        [256, 192, 7, 7]          249,336\n",
       "│    │    └─UniversalInvertedResidual: 3-17        [256, 192, 7, 7]          322,944\n",
       "│    │    └─UniversalInvertedResidual: 3-18        [256, 192, 7, 7]          319,872\n",
       "│    │    └─UniversalInvertedResidual: 3-19        [256, 192, 7, 7]          319,872\n",
       "│    │    └─UniversalInvertedResidual: 3-20        [256, 192, 7, 7]          148,800\n",
       "│    │    └─UniversalInvertedResidual: 3-21        [256, 192, 7, 7]          161,280\n",
       "│    │    └─UniversalInvertedResidual: 3-22        [256, 192, 7, 7]          148,800\n",
       "│    │    └─UniversalInvertedResidual: 3-23        [256, 192, 7, 7]          297,024\n",
       "│    │    └─MobileAttention: 3-24                  [256, 192, 7, 7]          123,456\n",
       "│    │    └─UniversalInvertedResidual: 3-25        [256, 192, 7, 7]          299,136\n",
       "│    │    └─MobileAttention: 3-26                  [256, 192, 7, 7]          123,456\n",
       "│    │    └─UniversalInvertedResidual: 3-27        [256, 192, 7, 7]          322,944\n",
       "│    │    └─MobileAttention: 3-28                  [256, 192, 7, 7]          123,456\n",
       "│    │    └─UniversalInvertedResidual: 3-29        [256, 192, 7, 7]          302,208\n",
       "│    │    └─MobileAttention: 3-30                  [256, 192, 7, 7]          123,456\n",
       "│    │    └─UniversalInvertedResidual: 3-31        [256, 192, 7, 7]          302,208\n",
       "│    └─Sequential: 2-7                             [256, 720, 7, 7]          --\n",
       "│    │    └─ConvBnAct: 3-32                        [256, 720, 7, 7]          139,680\n",
       "├─SelectAdaptivePool2d: 1-4                        [256, 720, 1, 1]          --\n",
       "│    └─AdaptiveAvgPool2d: 2-8                      [256, 720, 1, 1]          --\n",
       "│    └─Identity: 2-9                               [256, 720, 1, 1]          --\n",
       "├─Conv2d: 1-5                                      [256, 1280, 1, 1]         921,600\n",
       "├─BatchNormAct2d: 1-6                              [256, 1280, 1, 1]         2,560\n",
       "│    └─Identity: 2-10                              [256, 1280, 1, 1]         --\n",
       "│    └─ReLU: 2-11                                  [256, 1280, 1, 1]         --\n",
       "├─Identity: 1-7                                    [256, 1280, 1, 1]         --\n",
       "├─Flatten: 1-8                                     [256, 1280]               --\n",
       "├─Linear: 1-9                                      [256, 1000]               1,281,000\n",
       "====================================================================================================\n",
       "Total params: 7,313,528\n",
       "Trainable params: 7,313,528\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 160.07\n",
       "====================================================================================================\n",
       "Input size (MB): 154.14\n",
       "Forward/backward pass size (MB): 11854.23\n",
       "Params size (MB): 29.03\n",
       "Estimated Total Size (MB): 12037.40\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat_resnext26ts\n",
      "beit_base_patch16_224\n",
      "beit_base_patch16_384\n",
      "beit_large_patch16_224\n",
      "beit_large_patch16_384\n",
      "beit_large_patch16_512\n",
      "beitv2_base_patch16_224\n",
      "beitv2_large_patch16_224\n",
      "botnet26t_256\n",
      "botnet50ts_256\n",
      "caformer_b36\n",
      "caformer_m36\n",
      "caformer_s18\n",
      "caformer_s36\n",
      "cait_m36_384\n",
      "cait_m48_448\n",
      "cait_s24_224\n",
      "cait_s24_384\n",
      "cait_s36_384\n",
      "cait_xs24_384\n",
      "cait_xxs24_224\n",
      "cait_xxs24_384\n",
      "cait_xxs36_224\n",
      "cait_xxs36_384\n",
      "coat_lite_medium\n",
      "coat_lite_medium_384\n",
      "coat_lite_mini\n",
      "coat_lite_small\n",
      "coat_lite_tiny\n",
      "coat_mini\n",
      "coat_small\n",
      "coat_tiny\n",
      "coatnet_0_224\n",
      "coatnet_0_rw_224\n",
      "coatnet_1_224\n",
      "coatnet_1_rw_224\n",
      "coatnet_2_224\n",
      "coatnet_2_rw_224\n",
      "coatnet_3_224\n",
      "coatnet_3_rw_224\n",
      "coatnet_4_224\n",
      "coatnet_5_224\n",
      "coatnet_bn_0_rw_224\n",
      "coatnet_nano_cc_224\n",
      "coatnet_nano_rw_224\n",
      "coatnet_pico_rw_224\n",
      "coatnet_rmlp_0_rw_224\n",
      "coatnet_rmlp_1_rw2_224\n",
      "coatnet_rmlp_1_rw_224\n",
      "coatnet_rmlp_2_rw_224\n",
      "coatnet_rmlp_2_rw_384\n",
      "coatnet_rmlp_3_rw_224\n",
      "coatnet_rmlp_nano_rw_224\n",
      "coatnext_nano_rw_224\n",
      "convformer_b36\n",
      "convformer_m36\n",
      "convformer_s18\n",
      "convformer_s36\n",
      "convit_base\n",
      "convit_small\n",
      "convit_tiny\n",
      "convmixer_768_32\n",
      "convmixer_1024_20_ks9_p14\n",
      "convmixer_1536_20\n",
      "convnext_atto\n",
      "convnext_atto_ols\n",
      "convnext_base\n",
      "convnext_femto\n",
      "convnext_femto_ols\n",
      "convnext_large\n",
      "convnext_large_mlp\n",
      "convnext_nano\n",
      "convnext_nano_ols\n",
      "convnext_pico\n",
      "convnext_pico_ols\n",
      "convnext_small\n",
      "convnext_tiny\n",
      "convnext_tiny_hnf\n",
      "convnext_xlarge\n",
      "convnext_xxlarge\n",
      "convnextv2_atto\n",
      "convnextv2_base\n",
      "convnextv2_femto\n",
      "convnextv2_huge\n",
      "convnextv2_large\n",
      "convnextv2_nano\n",
      "convnextv2_pico\n",
      "convnextv2_small\n",
      "convnextv2_tiny\n",
      "crossvit_9_240\n",
      "crossvit_9_dagger_240\n",
      "crossvit_15_240\n",
      "crossvit_15_dagger_240\n",
      "crossvit_15_dagger_408\n",
      "crossvit_18_240\n",
      "crossvit_18_dagger_240\n",
      "crossvit_18_dagger_408\n",
      "crossvit_base_240\n",
      "crossvit_small_240\n",
      "crossvit_tiny_240\n",
      "cs3darknet_focus_l\n",
      "cs3darknet_focus_m\n",
      "cs3darknet_focus_s\n",
      "cs3darknet_focus_x\n",
      "cs3darknet_l\n",
      "cs3darknet_m\n",
      "cs3darknet_s\n",
      "cs3darknet_x\n",
      "cs3edgenet_x\n",
      "cs3se_edgenet_x\n",
      "cs3sedarknet_l\n",
      "cs3sedarknet_x\n",
      "cs3sedarknet_xdw\n",
      "cspdarknet53\n",
      "cspresnet50\n",
      "cspresnet50d\n",
      "cspresnet50w\n",
      "cspresnext50\n",
      "darknet17\n",
      "darknet21\n",
      "darknet53\n",
      "darknetaa53\n",
      "davit_base\n",
      "davit_base_fl\n",
      "davit_giant\n",
      "davit_huge\n",
      "davit_huge_fl\n",
      "davit_large\n",
      "davit_small\n",
      "davit_tiny\n",
      "deit3_base_patch16_224\n",
      "deit3_base_patch16_384\n",
      "deit3_huge_patch14_224\n",
      "deit3_large_patch16_224\n",
      "deit3_large_patch16_384\n",
      "deit3_medium_patch16_224\n",
      "deit3_small_patch16_224\n",
      "deit3_small_patch16_384\n",
      "deit_base_distilled_patch16_224\n",
      "deit_base_distilled_patch16_384\n",
      "deit_base_patch16_224\n",
      "deit_base_patch16_384\n",
      "deit_small_distilled_patch16_224\n",
      "deit_small_patch16_224\n",
      "deit_tiny_distilled_patch16_224\n",
      "deit_tiny_patch16_224\n",
      "densenet121\n",
      "densenet161\n",
      "densenet169\n",
      "densenet201\n",
      "densenet264d\n",
      "densenetblur121d\n",
      "dla34\n",
      "dla46_c\n",
      "dla46x_c\n",
      "dla60\n",
      "dla60_res2net\n",
      "dla60_res2next\n",
      "dla60x\n",
      "dla60x_c\n",
      "dla102\n",
      "dla102x\n",
      "dla102x2\n",
      "dla169\n",
      "dm_nfnet_f0\n",
      "dm_nfnet_f1\n",
      "dm_nfnet_f2\n",
      "dm_nfnet_f3\n",
      "dm_nfnet_f4\n",
      "dm_nfnet_f5\n",
      "dm_nfnet_f6\n",
      "dpn48b\n",
      "dpn68\n",
      "dpn68b\n",
      "dpn92\n",
      "dpn98\n",
      "dpn107\n",
      "dpn131\n",
      "eca_botnext26ts_256\n",
      "eca_halonext26ts\n",
      "eca_nfnet_l0\n",
      "eca_nfnet_l1\n",
      "eca_nfnet_l2\n",
      "eca_nfnet_l3\n",
      "eca_resnet33ts\n",
      "eca_resnext26ts\n",
      "eca_vovnet39b\n",
      "ecaresnet26t\n",
      "ecaresnet50d\n",
      "ecaresnet50d_pruned\n",
      "ecaresnet50t\n",
      "ecaresnet101d\n",
      "ecaresnet101d_pruned\n",
      "ecaresnet200d\n",
      "ecaresnet269d\n",
      "ecaresnetlight\n",
      "ecaresnext26t_32x4d\n",
      "ecaresnext50t_32x4d\n",
      "edgenext_base\n",
      "edgenext_small\n",
      "edgenext_small_rw\n",
      "edgenext_x_small\n",
      "edgenext_xx_small\n",
      "efficientformer_l1\n",
      "efficientformer_l3\n",
      "efficientformer_l7\n",
      "efficientformerv2_l\n",
      "efficientformerv2_s0\n",
      "efficientformerv2_s1\n",
      "efficientformerv2_s2\n",
      "efficientnet_b0\n",
      "efficientnet_b0_g8_gn\n",
      "efficientnet_b0_g16_evos\n",
      "efficientnet_b0_gn\n",
      "efficientnet_b1\n",
      "efficientnet_b1_pruned\n",
      "efficientnet_b2\n",
      "efficientnet_b2_pruned\n",
      "efficientnet_b3\n",
      "efficientnet_b3_g8_gn\n",
      "efficientnet_b3_gn\n",
      "efficientnet_b3_pruned\n",
      "efficientnet_b4\n",
      "efficientnet_b5\n",
      "efficientnet_b6\n",
      "efficientnet_b7\n",
      "efficientnet_b8\n",
      "efficientnet_blur_b0\n",
      "efficientnet_cc_b0_4e\n",
      "efficientnet_cc_b0_8e\n",
      "efficientnet_cc_b1_8e\n",
      "efficientnet_el\n",
      "efficientnet_el_pruned\n",
      "efficientnet_em\n",
      "efficientnet_es\n",
      "efficientnet_es_pruned\n",
      "efficientnet_h_b5\n",
      "efficientnet_l2\n",
      "efficientnet_lite0\n",
      "efficientnet_lite1\n",
      "efficientnet_lite2\n",
      "efficientnet_lite3\n",
      "efficientnet_lite4\n",
      "efficientnet_x_b3\n",
      "efficientnet_x_b5\n",
      "efficientnetv2_l\n",
      "efficientnetv2_m\n",
      "efficientnetv2_rw_m\n",
      "efficientnetv2_rw_s\n",
      "efficientnetv2_rw_t\n",
      "efficientnetv2_s\n",
      "efficientnetv2_xl\n",
      "efficientvit_b0\n",
      "efficientvit_b1\n",
      "efficientvit_b2\n",
      "efficientvit_b3\n",
      "efficientvit_l1\n",
      "efficientvit_l2\n",
      "efficientvit_l3\n",
      "efficientvit_m0\n",
      "efficientvit_m1\n",
      "efficientvit_m2\n",
      "efficientvit_m3\n",
      "efficientvit_m4\n",
      "efficientvit_m5\n",
      "ese_vovnet19b_dw\n",
      "ese_vovnet19b_slim\n",
      "ese_vovnet19b_slim_dw\n",
      "ese_vovnet39b\n",
      "ese_vovnet39b_evos\n",
      "ese_vovnet57b\n",
      "ese_vovnet99b\n",
      "eva02_base_patch14_224\n",
      "eva02_base_patch14_448\n",
      "eva02_base_patch16_clip_224\n",
      "eva02_enormous_patch14_clip_224\n",
      "eva02_large_patch14_224\n",
      "eva02_large_patch14_448\n",
      "eva02_large_patch14_clip_224\n",
      "eva02_large_patch14_clip_336\n",
      "eva02_small_patch14_224\n",
      "eva02_small_patch14_336\n",
      "eva02_tiny_patch14_224\n",
      "eva02_tiny_patch14_336\n",
      "eva_giant_patch14_224\n",
      "eva_giant_patch14_336\n",
      "eva_giant_patch14_560\n",
      "eva_giant_patch14_clip_224\n",
      "eva_large_patch14_196\n",
      "eva_large_patch14_336\n",
      "fastvit_ma36\n",
      "fastvit_mci0\n",
      "fastvit_mci1\n",
      "fastvit_mci2\n",
      "fastvit_s12\n",
      "fastvit_sa12\n",
      "fastvit_sa24\n",
      "fastvit_sa36\n",
      "fastvit_t8\n",
      "fastvit_t12\n",
      "fbnetc_100\n",
      "fbnetv3_b\n",
      "fbnetv3_d\n",
      "fbnetv3_g\n",
      "flexivit_base\n",
      "flexivit_large\n",
      "flexivit_small\n",
      "focalnet_base_lrf\n",
      "focalnet_base_srf\n",
      "focalnet_huge_fl3\n",
      "focalnet_huge_fl4\n",
      "focalnet_large_fl3\n",
      "focalnet_large_fl4\n",
      "focalnet_small_lrf\n",
      "focalnet_small_srf\n",
      "focalnet_tiny_lrf\n",
      "focalnet_tiny_srf\n",
      "focalnet_xlarge_fl3\n",
      "focalnet_xlarge_fl4\n",
      "gc_efficientnetv2_rw_t\n",
      "gcresnet33ts\n",
      "gcresnet50t\n",
      "gcresnext26ts\n",
      "gcresnext50ts\n",
      "gcvit_base\n",
      "gcvit_small\n",
      "gcvit_tiny\n",
      "gcvit_xtiny\n",
      "gcvit_xxtiny\n",
      "gernet_l\n",
      "gernet_m\n",
      "gernet_s\n",
      "ghostnet_050\n",
      "ghostnet_100\n",
      "ghostnet_130\n",
      "ghostnetv2_100\n",
      "ghostnetv2_130\n",
      "ghostnetv2_160\n",
      "gmixer_12_224\n",
      "gmixer_24_224\n",
      "gmlp_b16_224\n",
      "gmlp_s16_224\n",
      "gmlp_ti16_224\n",
      "halo2botnet50ts_256\n",
      "halonet26t\n",
      "halonet50ts\n",
      "halonet_h1\n",
      "haloregnetz_b\n",
      "hardcorenas_a\n",
      "hardcorenas_b\n",
      "hardcorenas_c\n",
      "hardcorenas_d\n",
      "hardcorenas_e\n",
      "hardcorenas_f\n",
      "hgnet_base\n",
      "hgnet_small\n",
      "hgnet_tiny\n",
      "hgnetv2_b0\n",
      "hgnetv2_b1\n",
      "hgnetv2_b2\n",
      "hgnetv2_b3\n",
      "hgnetv2_b4\n",
      "hgnetv2_b5\n",
      "hgnetv2_b6\n",
      "hiera_base_224\n",
      "hiera_base_abswin_256\n",
      "hiera_base_plus_224\n",
      "hiera_huge_224\n",
      "hiera_large_224\n",
      "hiera_small_224\n",
      "hiera_small_abswin_256\n",
      "hiera_tiny_224\n",
      "hieradet_small\n",
      "hrnet_w18\n",
      "hrnet_w18_small\n",
      "hrnet_w18_small_v2\n",
      "hrnet_w18_ssld\n",
      "hrnet_w30\n",
      "hrnet_w32\n",
      "hrnet_w40\n",
      "hrnet_w44\n",
      "hrnet_w48\n",
      "hrnet_w48_ssld\n",
      "hrnet_w64\n",
      "inception_next_base\n",
      "inception_next_small\n",
      "inception_next_tiny\n",
      "inception_resnet_v2\n",
      "inception_v3\n",
      "inception_v4\n",
      "lambda_resnet26rpt_256\n",
      "lambda_resnet26t\n",
      "lambda_resnet50ts\n",
      "lamhalobotnet50ts_256\n",
      "lcnet_035\n",
      "lcnet_050\n",
      "lcnet_075\n",
      "lcnet_100\n",
      "lcnet_150\n",
      "legacy_senet154\n",
      "legacy_seresnet18\n",
      "legacy_seresnet34\n",
      "legacy_seresnet50\n",
      "legacy_seresnet101\n",
      "legacy_seresnet152\n",
      "legacy_seresnext26_32x4d\n",
      "legacy_seresnext50_32x4d\n",
      "legacy_seresnext101_32x4d\n",
      "legacy_xception\n",
      "levit_128\n",
      "levit_128s\n",
      "levit_192\n",
      "levit_256\n",
      "levit_256d\n",
      "levit_384\n",
      "levit_384_s8\n",
      "levit_512\n",
      "levit_512_s8\n",
      "levit_512d\n",
      "levit_conv_128\n",
      "levit_conv_128s\n",
      "levit_conv_192\n",
      "levit_conv_256\n",
      "levit_conv_256d\n",
      "levit_conv_384\n",
      "levit_conv_384_s8\n",
      "levit_conv_512\n",
      "levit_conv_512_s8\n",
      "levit_conv_512d\n",
      "maxvit_base_tf_224\n",
      "maxvit_base_tf_384\n",
      "maxvit_base_tf_512\n",
      "maxvit_large_tf_224\n",
      "maxvit_large_tf_384\n",
      "maxvit_large_tf_512\n",
      "maxvit_nano_rw_256\n",
      "maxvit_pico_rw_256\n",
      "maxvit_rmlp_base_rw_224\n",
      "maxvit_rmlp_base_rw_384\n",
      "maxvit_rmlp_nano_rw_256\n",
      "maxvit_rmlp_pico_rw_256\n",
      "maxvit_rmlp_small_rw_224\n",
      "maxvit_rmlp_small_rw_256\n",
      "maxvit_rmlp_tiny_rw_256\n",
      "maxvit_small_tf_224\n",
      "maxvit_small_tf_384\n",
      "maxvit_small_tf_512\n",
      "maxvit_tiny_pm_256\n",
      "maxvit_tiny_rw_224\n",
      "maxvit_tiny_rw_256\n",
      "maxvit_tiny_tf_224\n",
      "maxvit_tiny_tf_384\n",
      "maxvit_tiny_tf_512\n",
      "maxvit_xlarge_tf_224\n",
      "maxvit_xlarge_tf_384\n",
      "maxvit_xlarge_tf_512\n",
      "maxxvit_rmlp_nano_rw_256\n",
      "maxxvit_rmlp_small_rw_256\n",
      "maxxvit_rmlp_tiny_rw_256\n",
      "maxxvitv2_nano_rw_256\n",
      "maxxvitv2_rmlp_base_rw_224\n",
      "maxxvitv2_rmlp_base_rw_384\n",
      "maxxvitv2_rmlp_large_rw_224\n",
      "mixer_b16_224\n",
      "mixer_b32_224\n",
      "mixer_l16_224\n",
      "mixer_l32_224\n",
      "mixer_s16_224\n",
      "mixer_s32_224\n",
      "mixnet_l\n",
      "mixnet_m\n",
      "mixnet_s\n",
      "mixnet_xl\n",
      "mixnet_xxl\n",
      "mnasnet_050\n",
      "mnasnet_075\n",
      "mnasnet_100\n",
      "mnasnet_140\n",
      "mnasnet_small\n",
      "mobilenet_edgetpu_100\n",
      "mobilenet_edgetpu_v2_l\n",
      "mobilenet_edgetpu_v2_m\n",
      "mobilenet_edgetpu_v2_s\n",
      "mobilenet_edgetpu_v2_xs\n",
      "mobilenetv1_100\n",
      "mobilenetv1_100h\n",
      "mobilenetv1_125\n",
      "mobilenetv2_035\n",
      "mobilenetv2_050\n",
      "mobilenetv2_075\n",
      "mobilenetv2_100\n",
      "mobilenetv2_110d\n",
      "mobilenetv2_120d\n",
      "mobilenetv2_140\n",
      "mobilenetv3_large_075\n",
      "mobilenetv3_large_100\n",
      "mobilenetv3_large_150d\n",
      "mobilenetv3_rw\n",
      "mobilenetv3_small_050\n",
      "mobilenetv3_small_075\n",
      "mobilenetv3_small_100\n",
      "mobilenetv4_conv_aa_large\n",
      "mobilenetv4_conv_aa_medium\n",
      "mobilenetv4_conv_blur_medium\n",
      "mobilenetv4_conv_large\n",
      "mobilenetv4_conv_medium\n",
      "mobilenetv4_conv_small\n",
      "mobilenetv4_hybrid_large\n",
      "mobilenetv4_hybrid_large_075\n",
      "mobilenetv4_hybrid_medium\n",
      "mobilenetv4_hybrid_medium_075\n",
      "mobileone_s0\n",
      "mobileone_s1\n",
      "mobileone_s2\n",
      "mobileone_s3\n",
      "mobileone_s4\n",
      "mobilevit_s\n",
      "mobilevit_xs\n",
      "mobilevit_xxs\n",
      "mobilevitv2_050\n",
      "mobilevitv2_075\n",
      "mobilevitv2_100\n",
      "mobilevitv2_125\n",
      "mobilevitv2_150\n",
      "mobilevitv2_175\n",
      "mobilevitv2_200\n",
      "mvitv2_base\n",
      "mvitv2_base_cls\n",
      "mvitv2_huge_cls\n",
      "mvitv2_large\n",
      "mvitv2_large_cls\n",
      "mvitv2_small\n",
      "mvitv2_small_cls\n",
      "mvitv2_tiny\n",
      "nasnetalarge\n",
      "nest_base\n",
      "nest_base_jx\n",
      "nest_small\n",
      "nest_small_jx\n",
      "nest_tiny\n",
      "nest_tiny_jx\n",
      "nextvit_base\n",
      "nextvit_large\n",
      "nextvit_small\n",
      "nf_ecaresnet26\n",
      "nf_ecaresnet50\n",
      "nf_ecaresnet101\n",
      "nf_regnet_b0\n",
      "nf_regnet_b1\n",
      "nf_regnet_b2\n",
      "nf_regnet_b3\n",
      "nf_regnet_b4\n",
      "nf_regnet_b5\n",
      "nf_resnet26\n",
      "nf_resnet50\n",
      "nf_resnet101\n",
      "nf_seresnet26\n",
      "nf_seresnet50\n",
      "nf_seresnet101\n",
      "nfnet_f0\n",
      "nfnet_f1\n",
      "nfnet_f2\n",
      "nfnet_f3\n",
      "nfnet_f4\n",
      "nfnet_f5\n",
      "nfnet_f6\n",
      "nfnet_f7\n",
      "nfnet_l0\n",
      "pit_b_224\n",
      "pit_b_distilled_224\n",
      "pit_s_224\n",
      "pit_s_distilled_224\n",
      "pit_ti_224\n",
      "pit_ti_distilled_224\n",
      "pit_xs_224\n",
      "pit_xs_distilled_224\n",
      "pnasnet5large\n",
      "poolformer_m36\n",
      "poolformer_m48\n",
      "poolformer_s12\n",
      "poolformer_s24\n",
      "poolformer_s36\n",
      "poolformerv2_m36\n",
      "poolformerv2_m48\n",
      "poolformerv2_s12\n",
      "poolformerv2_s24\n",
      "poolformerv2_s36\n",
      "pvt_v2_b0\n",
      "pvt_v2_b1\n",
      "pvt_v2_b2\n",
      "pvt_v2_b2_li\n",
      "pvt_v2_b3\n",
      "pvt_v2_b4\n",
      "pvt_v2_b5\n",
      "rdnet_base\n",
      "rdnet_large\n",
      "rdnet_small\n",
      "rdnet_tiny\n",
      "regnetv_040\n",
      "regnetv_064\n",
      "regnetx_002\n",
      "regnetx_004\n",
      "regnetx_004_tv\n",
      "regnetx_006\n",
      "regnetx_008\n",
      "regnetx_016\n",
      "regnetx_032\n",
      "regnetx_040\n",
      "regnetx_064\n",
      "regnetx_080\n",
      "regnetx_120\n",
      "regnetx_160\n",
      "regnetx_320\n",
      "regnety_002\n",
      "regnety_004\n",
      "regnety_006\n",
      "regnety_008\n",
      "regnety_008_tv\n",
      "regnety_016\n",
      "regnety_032\n",
      "regnety_040\n",
      "regnety_040_sgn\n",
      "regnety_064\n",
      "regnety_080\n",
      "regnety_080_tv\n",
      "regnety_120\n",
      "regnety_160\n",
      "regnety_320\n",
      "regnety_640\n",
      "regnety_1280\n",
      "regnety_2560\n",
      "regnetz_005\n",
      "regnetz_040\n",
      "regnetz_040_h\n",
      "regnetz_b16\n",
      "regnetz_b16_evos\n",
      "regnetz_c16\n",
      "regnetz_c16_evos\n",
      "regnetz_d8\n",
      "regnetz_d8_evos\n",
      "regnetz_d32\n",
      "regnetz_e8\n",
      "repghostnet_050\n",
      "repghostnet_058\n",
      "repghostnet_080\n",
      "repghostnet_100\n",
      "repghostnet_111\n",
      "repghostnet_130\n",
      "repghostnet_150\n",
      "repghostnet_200\n",
      "repvgg_a0\n",
      "repvgg_a1\n",
      "repvgg_a2\n",
      "repvgg_b0\n",
      "repvgg_b1\n",
      "repvgg_b1g4\n",
      "repvgg_b2\n",
      "repvgg_b2g4\n",
      "repvgg_b3\n",
      "repvgg_b3g4\n",
      "repvgg_d2se\n",
      "repvit_m0_9\n",
      "repvit_m1\n",
      "repvit_m1_0\n",
      "repvit_m1_1\n",
      "repvit_m1_5\n",
      "repvit_m2\n",
      "repvit_m2_3\n",
      "repvit_m3\n",
      "res2net50_14w_8s\n",
      "res2net50_26w_4s\n",
      "res2net50_26w_6s\n",
      "res2net50_26w_8s\n",
      "res2net50_48w_2s\n",
      "res2net50d\n",
      "res2net101_26w_4s\n",
      "res2net101d\n",
      "res2next50\n",
      "resmlp_12_224\n",
      "resmlp_24_224\n",
      "resmlp_36_224\n",
      "resmlp_big_24_224\n",
      "resnest14d\n",
      "resnest26d\n",
      "resnest50d\n",
      "resnest50d_1s4x24d\n",
      "resnest50d_4s2x40d\n",
      "resnest101e\n",
      "resnest200e\n",
      "resnest269e\n",
      "resnet10t\n",
      "resnet14t\n",
      "resnet18\n",
      "resnet18d\n",
      "resnet26\n",
      "resnet26d\n",
      "resnet26t\n",
      "resnet32ts\n",
      "resnet33ts\n",
      "resnet34\n",
      "resnet34d\n",
      "resnet50\n",
      "resnet50_clip\n",
      "resnet50_clip_gap\n",
      "resnet50_gn\n",
      "resnet50_mlp\n",
      "resnet50c\n",
      "resnet50d\n",
      "resnet50s\n",
      "resnet50t\n",
      "resnet50x4_clip\n",
      "resnet50x4_clip_gap\n",
      "resnet50x16_clip\n",
      "resnet50x16_clip_gap\n",
      "resnet50x64_clip\n",
      "resnet50x64_clip_gap\n",
      "resnet51q\n",
      "resnet61q\n",
      "resnet101\n",
      "resnet101_clip\n",
      "resnet101_clip_gap\n",
      "resnet101c\n",
      "resnet101d\n",
      "resnet101s\n",
      "resnet152\n",
      "resnet152c\n",
      "resnet152d\n",
      "resnet152s\n",
      "resnet200\n",
      "resnet200d\n",
      "resnetaa34d\n",
      "resnetaa50\n",
      "resnetaa50d\n",
      "resnetaa101d\n",
      "resnetblur18\n",
      "resnetblur50\n",
      "resnetblur50d\n",
      "resnetblur101d\n",
      "resnetrs50\n",
      "resnetrs101\n",
      "resnetrs152\n",
      "resnetrs200\n",
      "resnetrs270\n",
      "resnetrs350\n",
      "resnetrs420\n",
      "resnetv2_50\n",
      "resnetv2_50d\n",
      "resnetv2_50d_evos\n",
      "resnetv2_50d_frn\n",
      "resnetv2_50d_gn\n",
      "resnetv2_50t\n",
      "resnetv2_50x1_bit\n",
      "resnetv2_50x3_bit\n",
      "resnetv2_101\n",
      "resnetv2_101d\n",
      "resnetv2_101x1_bit\n",
      "resnetv2_101x3_bit\n",
      "resnetv2_152\n",
      "resnetv2_152d\n",
      "resnetv2_152x2_bit\n",
      "resnetv2_152x4_bit\n",
      "resnext26ts\n",
      "resnext50_32x4d\n",
      "resnext50d_32x4d\n",
      "resnext101_32x4d\n",
      "resnext101_32x8d\n",
      "resnext101_32x16d\n",
      "resnext101_32x32d\n",
      "resnext101_64x4d\n",
      "rexnet_100\n",
      "rexnet_130\n",
      "rexnet_150\n",
      "rexnet_200\n",
      "rexnet_300\n",
      "rexnetr_100\n",
      "rexnetr_130\n",
      "rexnetr_150\n",
      "rexnetr_200\n",
      "rexnetr_300\n",
      "sam2_hiera_base_plus\n",
      "sam2_hiera_large\n",
      "sam2_hiera_small\n",
      "sam2_hiera_tiny\n",
      "samvit_base_patch16\n",
      "samvit_base_patch16_224\n",
      "samvit_huge_patch16\n",
      "samvit_large_patch16\n",
      "sebotnet33ts_256\n",
      "sedarknet21\n",
      "sehalonet33ts\n",
      "selecsls42\n",
      "selecsls42b\n",
      "selecsls60\n",
      "selecsls60b\n",
      "selecsls84\n",
      "semnasnet_050\n",
      "semnasnet_075\n",
      "semnasnet_100\n",
      "semnasnet_140\n",
      "senet154\n",
      "sequencer2d_l\n",
      "sequencer2d_m\n",
      "sequencer2d_s\n",
      "seresnet18\n",
      "seresnet33ts\n",
      "seresnet34\n",
      "seresnet50\n",
      "seresnet50t\n",
      "seresnet101\n",
      "seresnet152\n",
      "seresnet152d\n",
      "seresnet200d\n",
      "seresnet269d\n",
      "seresnetaa50d\n",
      "seresnext26d_32x4d\n",
      "seresnext26t_32x4d\n",
      "seresnext26ts\n",
      "seresnext50_32x4d\n",
      "seresnext101_32x4d\n",
      "seresnext101_32x8d\n",
      "seresnext101_64x4d\n",
      "seresnext101d_32x8d\n",
      "seresnextaa101d_32x8d\n",
      "seresnextaa201d_32x8d\n",
      "skresnet18\n",
      "skresnet34\n",
      "skresnet50\n",
      "skresnet50d\n",
      "skresnext50_32x4d\n",
      "spnasnet_100\n",
      "swin_base_patch4_window7_224\n",
      "swin_base_patch4_window12_384\n",
      "swin_large_patch4_window7_224\n",
      "swin_large_patch4_window12_384\n",
      "swin_s3_base_224\n",
      "swin_s3_small_224\n",
      "swin_s3_tiny_224\n",
      "swin_small_patch4_window7_224\n",
      "swin_tiny_patch4_window7_224\n",
      "swinv2_base_window8_256\n",
      "swinv2_base_window12_192\n",
      "swinv2_base_window12to16_192to256\n",
      "swinv2_base_window12to24_192to384\n",
      "swinv2_base_window16_256\n",
      "swinv2_cr_base_224\n",
      "swinv2_cr_base_384\n",
      "swinv2_cr_base_ns_224\n",
      "swinv2_cr_giant_224\n",
      "swinv2_cr_giant_384\n",
      "swinv2_cr_huge_224\n",
      "swinv2_cr_huge_384\n",
      "swinv2_cr_large_224\n",
      "swinv2_cr_large_384\n",
      "swinv2_cr_small_224\n",
      "swinv2_cr_small_384\n",
      "swinv2_cr_small_ns_224\n",
      "swinv2_cr_small_ns_256\n",
      "swinv2_cr_tiny_224\n",
      "swinv2_cr_tiny_384\n",
      "swinv2_cr_tiny_ns_224\n",
      "swinv2_large_window12_192\n",
      "swinv2_large_window12to16_192to256\n",
      "swinv2_large_window12to24_192to384\n",
      "swinv2_small_window8_256\n",
      "swinv2_small_window16_256\n",
      "swinv2_tiny_window8_256\n",
      "swinv2_tiny_window16_256\n",
      "test_byobnet\n",
      "test_efficientnet\n",
      "test_vit\n",
      "tf_efficientnet_b0\n",
      "tf_efficientnet_b1\n",
      "tf_efficientnet_b2\n",
      "tf_efficientnet_b3\n",
      "tf_efficientnet_b4\n",
      "tf_efficientnet_b5\n",
      "tf_efficientnet_b6\n",
      "tf_efficientnet_b7\n",
      "tf_efficientnet_b8\n",
      "tf_efficientnet_cc_b0_4e\n",
      "tf_efficientnet_cc_b0_8e\n",
      "tf_efficientnet_cc_b1_8e\n",
      "tf_efficientnet_el\n",
      "tf_efficientnet_em\n",
      "tf_efficientnet_es\n",
      "tf_efficientnet_l2\n",
      "tf_efficientnet_lite0\n",
      "tf_efficientnet_lite1\n",
      "tf_efficientnet_lite2\n",
      "tf_efficientnet_lite3\n",
      "tf_efficientnet_lite4\n",
      "tf_efficientnetv2_b0\n",
      "tf_efficientnetv2_b1\n",
      "tf_efficientnetv2_b2\n",
      "tf_efficientnetv2_b3\n",
      "tf_efficientnetv2_l\n",
      "tf_efficientnetv2_m\n",
      "tf_efficientnetv2_s\n",
      "tf_efficientnetv2_xl\n",
      "tf_mixnet_l\n",
      "tf_mixnet_m\n",
      "tf_mixnet_s\n",
      "tf_mobilenetv3_large_075\n",
      "tf_mobilenetv3_large_100\n",
      "tf_mobilenetv3_large_minimal_100\n",
      "tf_mobilenetv3_small_075\n",
      "tf_mobilenetv3_small_100\n",
      "tf_mobilenetv3_small_minimal_100\n",
      "tiny_vit_5m_224\n",
      "tiny_vit_11m_224\n",
      "tiny_vit_21m_224\n",
      "tiny_vit_21m_384\n",
      "tiny_vit_21m_512\n",
      "tinynet_a\n",
      "tinynet_b\n",
      "tinynet_c\n",
      "tinynet_d\n",
      "tinynet_e\n",
      "tnt_b_patch16_224\n",
      "tnt_s_patch16_224\n",
      "tresnet_l\n",
      "tresnet_m\n",
      "tresnet_v2_l\n",
      "tresnet_xl\n",
      "twins_pcpvt_base\n",
      "twins_pcpvt_large\n",
      "twins_pcpvt_small\n",
      "twins_svt_base\n",
      "twins_svt_large\n",
      "twins_svt_small\n",
      "vgg11\n",
      "vgg11_bn\n",
      "vgg13\n",
      "vgg13_bn\n",
      "vgg16\n",
      "vgg16_bn\n",
      "vgg19\n",
      "vgg19_bn\n",
      "visformer_small\n",
      "visformer_tiny\n",
      "vit_base_mci_224\n",
      "vit_base_patch8_224\n",
      "vit_base_patch14_dinov2\n",
      "vit_base_patch14_reg4_dinov2\n",
      "vit_base_patch16_18x2_224\n",
      "vit_base_patch16_224\n",
      "vit_base_patch16_224_miil\n",
      "vit_base_patch16_384\n",
      "vit_base_patch16_clip_224\n",
      "vit_base_patch16_clip_384\n",
      "vit_base_patch16_clip_quickgelu_224\n",
      "vit_base_patch16_gap_224\n",
      "vit_base_patch16_plus_240\n",
      "vit_base_patch16_reg4_gap_256\n",
      "vit_base_patch16_rope_reg1_gap_256\n",
      "vit_base_patch16_rpn_224\n",
      "vit_base_patch16_siglip_224\n",
      "vit_base_patch16_siglip_256\n",
      "vit_base_patch16_siglip_384\n",
      "vit_base_patch16_siglip_512\n",
      "vit_base_patch16_siglip_gap_224\n",
      "vit_base_patch16_siglip_gap_256\n",
      "vit_base_patch16_siglip_gap_384\n",
      "vit_base_patch16_siglip_gap_512\n",
      "vit_base_patch16_xp_224\n",
      "vit_base_patch32_224\n",
      "vit_base_patch32_384\n",
      "vit_base_patch32_clip_224\n",
      "vit_base_patch32_clip_256\n",
      "vit_base_patch32_clip_384\n",
      "vit_base_patch32_clip_448\n",
      "vit_base_patch32_clip_quickgelu_224\n",
      "vit_base_patch32_plus_256\n",
      "vit_base_r26_s32_224\n",
      "vit_base_r50_s16_224\n",
      "vit_base_r50_s16_384\n",
      "vit_base_resnet26d_224\n",
      "vit_base_resnet50d_224\n",
      "vit_betwixt_patch16_gap_256\n",
      "vit_betwixt_patch16_reg1_gap_256\n",
      "vit_betwixt_patch16_reg4_gap_256\n",
      "vit_betwixt_patch16_reg4_gap_384\n",
      "vit_betwixt_patch16_rope_reg4_gap_256\n",
      "vit_betwixt_patch32_clip_224\n",
      "vit_giant_patch14_224\n",
      "vit_giant_patch14_clip_224\n",
      "vit_giant_patch14_dinov2\n",
      "vit_giant_patch14_reg4_dinov2\n",
      "vit_giant_patch16_gap_224\n",
      "vit_gigantic_patch14_224\n",
      "vit_gigantic_patch14_clip_224\n",
      "vit_huge_patch14_224\n",
      "vit_huge_patch14_clip_224\n",
      "vit_huge_patch14_clip_336\n",
      "vit_huge_patch14_clip_378\n",
      "vit_huge_patch14_clip_quickgelu_224\n",
      "vit_huge_patch14_clip_quickgelu_378\n",
      "vit_huge_patch14_gap_224\n",
      "vit_huge_patch14_xp_224\n",
      "vit_huge_patch16_gap_448\n",
      "vit_large_patch14_224\n",
      "vit_large_patch14_clip_224\n",
      "vit_large_patch14_clip_336\n",
      "vit_large_patch14_clip_quickgelu_224\n",
      "vit_large_patch14_clip_quickgelu_336\n",
      "vit_large_patch14_dinov2\n",
      "vit_large_patch14_reg4_dinov2\n",
      "vit_large_patch14_xp_224\n",
      "vit_large_patch16_224\n",
      "vit_large_patch16_384\n",
      "vit_large_patch16_siglip_256\n",
      "vit_large_patch16_siglip_384\n",
      "vit_large_patch16_siglip_gap_256\n",
      "vit_large_patch16_siglip_gap_384\n",
      "vit_large_patch32_224\n",
      "vit_large_patch32_384\n",
      "vit_large_r50_s32_224\n",
      "vit_large_r50_s32_384\n",
      "vit_little_patch16_reg1_gap_256\n",
      "vit_little_patch16_reg4_gap_256\n",
      "vit_medium_patch16_clip_224\n",
      "vit_medium_patch16_gap_240\n",
      "vit_medium_patch16_gap_256\n",
      "vit_medium_patch16_gap_384\n",
      "vit_medium_patch16_reg1_gap_256\n",
      "vit_medium_patch16_reg4_gap_256\n",
      "vit_medium_patch16_rope_reg1_gap_256\n",
      "vit_medium_patch32_clip_224\n",
      "vit_mediumd_patch16_reg4_gap_256\n",
      "vit_mediumd_patch16_reg4_gap_384\n",
      "vit_mediumd_patch16_rope_reg1_gap_256\n",
      "vit_pwee_patch16_reg1_gap_256\n",
      "vit_relpos_base_patch16_224\n",
      "vit_relpos_base_patch16_cls_224\n",
      "vit_relpos_base_patch16_clsgap_224\n",
      "vit_relpos_base_patch16_plus_240\n",
      "vit_relpos_base_patch16_rpn_224\n",
      "vit_relpos_base_patch32_plus_rpn_256\n",
      "vit_relpos_medium_patch16_224\n",
      "vit_relpos_medium_patch16_cls_224\n",
      "vit_relpos_medium_patch16_rpn_224\n",
      "vit_relpos_small_patch16_224\n",
      "vit_relpos_small_patch16_rpn_224\n",
      "vit_small_patch8_224\n",
      "vit_small_patch14_dinov2\n",
      "vit_small_patch14_reg4_dinov2\n",
      "vit_small_patch16_18x2_224\n",
      "vit_small_patch16_36x1_224\n",
      "vit_small_patch16_224\n",
      "vit_small_patch16_384\n",
      "vit_small_patch32_224\n",
      "vit_small_patch32_384\n",
      "vit_small_r26_s32_224\n",
      "vit_small_r26_s32_384\n",
      "vit_small_resnet26d_224\n",
      "vit_small_resnet50d_s16_224\n",
      "vit_so150m_patch16_reg4_gap_256\n",
      "vit_so150m_patch16_reg4_map_256\n",
      "vit_so400m_patch14_siglip_224\n",
      "vit_so400m_patch14_siglip_384\n",
      "vit_so400m_patch14_siglip_gap_224\n",
      "vit_so400m_patch14_siglip_gap_384\n",
      "vit_so400m_patch14_siglip_gap_448\n",
      "vit_so400m_patch14_siglip_gap_896\n",
      "vit_srelpos_medium_patch16_224\n",
      "vit_srelpos_small_patch16_224\n",
      "vit_tiny_patch16_224\n",
      "vit_tiny_patch16_384\n",
      "vit_tiny_r_s16_p8_224\n",
      "vit_tiny_r_s16_p8_384\n",
      "vit_wee_patch16_reg1_gap_256\n",
      "vit_xsmall_patch16_clip_224\n",
      "vitamin_base_224\n",
      "vitamin_large2_224\n",
      "vitamin_large2_256\n",
      "vitamin_large2_336\n",
      "vitamin_large2_384\n",
      "vitamin_large_224\n",
      "vitamin_large_256\n",
      "vitamin_large_336\n",
      "vitamin_large_384\n",
      "vitamin_small_224\n",
      "vitamin_xlarge_256\n",
      "vitamin_xlarge_336\n",
      "vitamin_xlarge_384\n",
      "volo_d1_224\n",
      "volo_d1_384\n",
      "volo_d2_224\n",
      "volo_d2_384\n",
      "volo_d3_224\n",
      "volo_d3_448\n",
      "volo_d4_224\n",
      "volo_d4_448\n",
      "volo_d5_224\n",
      "volo_d5_448\n",
      "volo_d5_512\n",
      "vovnet39a\n",
      "vovnet57a\n",
      "wide_resnet50_2\n",
      "wide_resnet101_2\n",
      "xception41\n",
      "xception41p\n",
      "xception65\n",
      "xception65p\n",
      "xception71\n",
      "xcit_large_24_p8_224\n",
      "xcit_large_24_p8_384\n",
      "xcit_large_24_p16_224\n",
      "xcit_large_24_p16_384\n",
      "xcit_medium_24_p8_224\n",
      "xcit_medium_24_p8_384\n",
      "xcit_medium_24_p16_224\n",
      "xcit_medium_24_p16_384\n",
      "xcit_nano_12_p8_224\n",
      "xcit_nano_12_p8_384\n",
      "xcit_nano_12_p16_224\n",
      "xcit_nano_12_p16_384\n",
      "xcit_small_12_p8_224\n",
      "xcit_small_12_p8_384\n",
      "xcit_small_12_p16_224\n",
      "xcit_small_12_p16_384\n",
      "xcit_small_24_p8_224\n",
      "xcit_small_24_p8_384\n",
      "xcit_small_24_p16_224\n",
      "xcit_small_24_p16_384\n",
      "xcit_tiny_12_p8_224\n",
      "xcit_tiny_12_p8_384\n",
      "xcit_tiny_12_p16_224\n",
      "xcit_tiny_12_p16_384\n",
      "xcit_tiny_24_p8_224\n",
      "xcit_tiny_24_p8_384\n",
      "xcit_tiny_24_p16_224\n",
      "xcit_tiny_24_p16_384\n"
     ]
    }
   ],
   "source": [
    "for i in timm.list_models():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VisionTransformer                        [1, 10]                   19,584\n",
       "├─PatchEmbed: 1-1                        [1, 49, 384]              --\n",
       "│    └─Conv2d: 2-1                       [1, 384, 7, 7]            1,180,032\n",
       "│    └─Identity: 2-2                     [1, 49, 384]              --\n",
       "├─Dropout: 1-2                           [1, 50, 384]              --\n",
       "├─Identity: 1-3                          [1, 50, 384]              --\n",
       "├─Identity: 1-4                          [1, 50, 384]              --\n",
       "├─Sequential: 1-5                        [1, 50, 384]              --\n",
       "│    └─Block: 2-3                        [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-1               [1, 50, 384]              768\n",
       "│    │    └─Attention: 3-2               [1, 50, 384]              591,360\n",
       "│    │    └─Identity: 3-3                [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-4                [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-5               [1, 50, 384]              768\n",
       "│    │    └─Mlp: 3-6                     [1, 50, 384]              1,181,568\n",
       "│    │    └─Identity: 3-7                [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-8                [1, 50, 384]              --\n",
       "│    └─Block: 2-4                        [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-9               [1, 50, 384]              768\n",
       "│    │    └─Attention: 3-10              [1, 50, 384]              591,360\n",
       "│    │    └─Identity: 3-11               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-12               [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-13              [1, 50, 384]              768\n",
       "│    │    └─Mlp: 3-14                    [1, 50, 384]              1,181,568\n",
       "│    │    └─Identity: 3-15               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-16               [1, 50, 384]              --\n",
       "│    └─Block: 2-5                        [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-17              [1, 50, 384]              768\n",
       "│    │    └─Attention: 3-18              [1, 50, 384]              591,360\n",
       "│    │    └─Identity: 3-19               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-20               [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-21              [1, 50, 384]              768\n",
       "│    │    └─Mlp: 3-22                    [1, 50, 384]              1,181,568\n",
       "│    │    └─Identity: 3-23               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-24               [1, 50, 384]              --\n",
       "│    └─Block: 2-6                        [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-25              [1, 50, 384]              768\n",
       "│    │    └─Attention: 3-26              [1, 50, 384]              591,360\n",
       "│    │    └─Identity: 3-27               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-28               [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-29              [1, 50, 384]              768\n",
       "│    │    └─Mlp: 3-30                    [1, 50, 384]              1,181,568\n",
       "│    │    └─Identity: 3-31               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-32               [1, 50, 384]              --\n",
       "│    └─Block: 2-7                        [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-33              [1, 50, 384]              768\n",
       "│    │    └─Attention: 3-34              [1, 50, 384]              591,360\n",
       "│    │    └─Identity: 3-35               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-36               [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-37              [1, 50, 384]              768\n",
       "│    │    └─Mlp: 3-38                    [1, 50, 384]              1,181,568\n",
       "│    │    └─Identity: 3-39               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-40               [1, 50, 384]              --\n",
       "│    └─Block: 2-8                        [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-41              [1, 50, 384]              768\n",
       "│    │    └─Attention: 3-42              [1, 50, 384]              591,360\n",
       "│    │    └─Identity: 3-43               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-44               [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-45              [1, 50, 384]              768\n",
       "│    │    └─Mlp: 3-46                    [1, 50, 384]              1,181,568\n",
       "│    │    └─Identity: 3-47               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-48               [1, 50, 384]              --\n",
       "│    └─Block: 2-9                        [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-49              [1, 50, 384]              768\n",
       "│    │    └─Attention: 3-50              [1, 50, 384]              591,360\n",
       "│    │    └─Identity: 3-51               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-52               [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-53              [1, 50, 384]              768\n",
       "│    │    └─Mlp: 3-54                    [1, 50, 384]              1,181,568\n",
       "│    │    └─Identity: 3-55               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-56               [1, 50, 384]              --\n",
       "│    └─Block: 2-10                       [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-57              [1, 50, 384]              768\n",
       "│    │    └─Attention: 3-58              [1, 50, 384]              591,360\n",
       "│    │    └─Identity: 3-59               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-60               [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-61              [1, 50, 384]              768\n",
       "│    │    └─Mlp: 3-62                    [1, 50, 384]              1,181,568\n",
       "│    │    └─Identity: 3-63               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-64               [1, 50, 384]              --\n",
       "│    └─Block: 2-11                       [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-65              [1, 50, 384]              768\n",
       "│    │    └─Attention: 3-66              [1, 50, 384]              591,360\n",
       "│    │    └─Identity: 3-67               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-68               [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-69              [1, 50, 384]              768\n",
       "│    │    └─Mlp: 3-70                    [1, 50, 384]              1,181,568\n",
       "│    │    └─Identity: 3-71               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-72               [1, 50, 384]              --\n",
       "│    └─Block: 2-12                       [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-73              [1, 50, 384]              768\n",
       "│    │    └─Attention: 3-74              [1, 50, 384]              591,360\n",
       "│    │    └─Identity: 3-75               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-76               [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-77              [1, 50, 384]              768\n",
       "│    │    └─Mlp: 3-78                    [1, 50, 384]              1,181,568\n",
       "│    │    └─Identity: 3-79               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-80               [1, 50, 384]              --\n",
       "│    └─Block: 2-13                       [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-81              [1, 50, 384]              768\n",
       "│    │    └─Attention: 3-82              [1, 50, 384]              591,360\n",
       "│    │    └─Identity: 3-83               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-84               [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-85              [1, 50, 384]              768\n",
       "│    │    └─Mlp: 3-86                    [1, 50, 384]              1,181,568\n",
       "│    │    └─Identity: 3-87               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-88               [1, 50, 384]              --\n",
       "│    └─Block: 2-14                       [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-89              [1, 50, 384]              768\n",
       "│    │    └─Attention: 3-90              [1, 50, 384]              591,360\n",
       "│    │    └─Identity: 3-91               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-92               [1, 50, 384]              --\n",
       "│    │    └─LayerNorm: 3-93              [1, 50, 384]              768\n",
       "│    │    └─Mlp: 3-94                    [1, 50, 384]              1,181,568\n",
       "│    │    └─Identity: 3-95               [1, 50, 384]              --\n",
       "│    │    └─Identity: 3-96               [1, 50, 384]              --\n",
       "├─LayerNorm: 1-6                         [1, 50, 384]              768\n",
       "├─Identity: 1-7                          [1, 384]                  --\n",
       "├─Dropout: 1-8                           [1, 384]                  --\n",
       "├─Linear: 1-9                            [1, 10]                   3,850\n",
       "==========================================================================================\n",
       "Total params: 22,497,802\n",
       "Trainable params: 22,497,802\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 79.12\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 20.58\n",
       "Params size (MB): 89.91\n",
       "Estimated Total Size (MB): 111.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self, exp_name: str, model_type: str, model_name: str, runs: int, epochs: int, batch_size: int, learning_rate: float):\n",
    "        self.exp_name = exp_name\n",
    "        self.model_type = model_type\n",
    "        self.model_name = model_name\n",
    "        self.runs = runs\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "cfg = ModelConfig(\n",
    "        exp_name = \"augreg\",\n",
    "        model_type = \"vit\",\n",
    "        model_name = \"vit_small_patch32_224.augreg_in21k_ft_in1k\",\n",
    "        runs = 50,\n",
    "        epochs = 300,\n",
    "        batch_size = 64,\n",
    "        learning_rate = 1e-3\n",
    "    )\n",
    "\n",
    "model = timm.create_model(cfg.model_name, pretrained=True, num_classes=10)\n",
    "# sample_input = torch.randn(1, 3, 224, 224)  # 單個樣本的輸入\n",
    "# output = model(sample_input)\n",
    "# print(output.shape)\n",
    "summary(model, input_size=(1, 3, 224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VisionTransformerDistilled               [1, 10]                   38,400\n",
       "├─PatchEmbed: 1-1                        [1, 196, 192]             --\n",
       "│    └─Conv2d: 2-1                       [1, 192, 14, 14]          147,648\n",
       "│    └─Identity: 2-2                     [1, 196, 192]             --\n",
       "├─Dropout: 1-2                           [1, 198, 192]             --\n",
       "├─Identity: 1-3                          [1, 198, 192]             --\n",
       "├─Identity: 1-4                          [1, 198, 192]             --\n",
       "├─Sequential: 1-5                        [1, 198, 192]             --\n",
       "│    └─Block: 2-3                        [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-1               [1, 198, 192]             384\n",
       "│    │    └─Attention: 3-2               [1, 198, 192]             148,224\n",
       "│    │    └─Identity: 3-3                [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-4                [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-5               [1, 198, 192]             384\n",
       "│    │    └─Mlp: 3-6                     [1, 198, 192]             295,872\n",
       "│    │    └─Identity: 3-7                [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-8                [1, 198, 192]             --\n",
       "│    └─Block: 2-4                        [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-9               [1, 198, 192]             384\n",
       "│    │    └─Attention: 3-10              [1, 198, 192]             148,224\n",
       "│    │    └─Identity: 3-11               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-12               [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-13              [1, 198, 192]             384\n",
       "│    │    └─Mlp: 3-14                    [1, 198, 192]             295,872\n",
       "│    │    └─Identity: 3-15               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-16               [1, 198, 192]             --\n",
       "│    └─Block: 2-5                        [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-17              [1, 198, 192]             384\n",
       "│    │    └─Attention: 3-18              [1, 198, 192]             148,224\n",
       "│    │    └─Identity: 3-19               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-20               [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-21              [1, 198, 192]             384\n",
       "│    │    └─Mlp: 3-22                    [1, 198, 192]             295,872\n",
       "│    │    └─Identity: 3-23               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-24               [1, 198, 192]             --\n",
       "│    └─Block: 2-6                        [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-25              [1, 198, 192]             384\n",
       "│    │    └─Attention: 3-26              [1, 198, 192]             148,224\n",
       "│    │    └─Identity: 3-27               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-28               [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-29              [1, 198, 192]             384\n",
       "│    │    └─Mlp: 3-30                    [1, 198, 192]             295,872\n",
       "│    │    └─Identity: 3-31               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-32               [1, 198, 192]             --\n",
       "│    └─Block: 2-7                        [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-33              [1, 198, 192]             384\n",
       "│    │    └─Attention: 3-34              [1, 198, 192]             148,224\n",
       "│    │    └─Identity: 3-35               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-36               [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-37              [1, 198, 192]             384\n",
       "│    │    └─Mlp: 3-38                    [1, 198, 192]             295,872\n",
       "│    │    └─Identity: 3-39               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-40               [1, 198, 192]             --\n",
       "│    └─Block: 2-8                        [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-41              [1, 198, 192]             384\n",
       "│    │    └─Attention: 3-42              [1, 198, 192]             148,224\n",
       "│    │    └─Identity: 3-43               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-44               [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-45              [1, 198, 192]             384\n",
       "│    │    └─Mlp: 3-46                    [1, 198, 192]             295,872\n",
       "│    │    └─Identity: 3-47               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-48               [1, 198, 192]             --\n",
       "│    └─Block: 2-9                        [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-49              [1, 198, 192]             384\n",
       "│    │    └─Attention: 3-50              [1, 198, 192]             148,224\n",
       "│    │    └─Identity: 3-51               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-52               [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-53              [1, 198, 192]             384\n",
       "│    │    └─Mlp: 3-54                    [1, 198, 192]             295,872\n",
       "│    │    └─Identity: 3-55               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-56               [1, 198, 192]             --\n",
       "│    └─Block: 2-10                       [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-57              [1, 198, 192]             384\n",
       "│    │    └─Attention: 3-58              [1, 198, 192]             148,224\n",
       "│    │    └─Identity: 3-59               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-60               [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-61              [1, 198, 192]             384\n",
       "│    │    └─Mlp: 3-62                    [1, 198, 192]             295,872\n",
       "│    │    └─Identity: 3-63               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-64               [1, 198, 192]             --\n",
       "│    └─Block: 2-11                       [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-65              [1, 198, 192]             384\n",
       "│    │    └─Attention: 3-66              [1, 198, 192]             148,224\n",
       "│    │    └─Identity: 3-67               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-68               [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-69              [1, 198, 192]             384\n",
       "│    │    └─Mlp: 3-70                    [1, 198, 192]             295,872\n",
       "│    │    └─Identity: 3-71               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-72               [1, 198, 192]             --\n",
       "│    └─Block: 2-12                       [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-73              [1, 198, 192]             384\n",
       "│    │    └─Attention: 3-74              [1, 198, 192]             148,224\n",
       "│    │    └─Identity: 3-75               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-76               [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-77              [1, 198, 192]             384\n",
       "│    │    └─Mlp: 3-78                    [1, 198, 192]             295,872\n",
       "│    │    └─Identity: 3-79               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-80               [1, 198, 192]             --\n",
       "│    └─Block: 2-13                       [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-81              [1, 198, 192]             384\n",
       "│    │    └─Attention: 3-82              [1, 198, 192]             148,224\n",
       "│    │    └─Identity: 3-83               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-84               [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-85              [1, 198, 192]             384\n",
       "│    │    └─Mlp: 3-86                    [1, 198, 192]             295,872\n",
       "│    │    └─Identity: 3-87               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-88               [1, 198, 192]             --\n",
       "│    └─Block: 2-14                       [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-89              [1, 198, 192]             384\n",
       "│    │    └─Attention: 3-90              [1, 198, 192]             148,224\n",
       "│    │    └─Identity: 3-91               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-92               [1, 198, 192]             --\n",
       "│    │    └─LayerNorm: 3-93              [1, 198, 192]             384\n",
       "│    │    └─Mlp: 3-94                    [1, 198, 192]             295,872\n",
       "│    │    └─Identity: 3-95               [1, 198, 192]             --\n",
       "│    │    └─Identity: 3-96               [1, 198, 192]             --\n",
       "├─LayerNorm: 1-6                         [1, 198, 192]             384\n",
       "├─Linear: 1-7                            [1, 10]                   1,930\n",
       "├─Linear: 1-8                            [1, 10]                   1,930\n",
       "==========================================================================================\n",
       "Total params: 5,528,660\n",
       "Trainable params: 5,528,660\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 34.28\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 40.75\n",
       "Params size (MB): 21.96\n",
       "Estimated Total Size (MB): 63.31\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = ModelConfig(\n",
    "        exp_name = \"augreg\",\n",
    "        model_type = \"vit\",\n",
    "        model_name = \"deit_tiny_distilled_patch16_224.fb_in1k\",\n",
    "        runs = 50,\n",
    "        epochs = 300,\n",
    "        batch_size = 64,\n",
    "        learning_rate = 1e-3\n",
    "    )\n",
    "\n",
    "model = timm.create_model(cfg.model_name, pretrained=True, num_classes=10)\n",
    "# sample_input = torch.randn(1, 3, 224, 224)  # 單個樣本的輸入\n",
    "# output = model(sample_input)\n",
    "# print(output.shape)\n",
    "summary(model, input_size=(1, 3, 224, 224))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
