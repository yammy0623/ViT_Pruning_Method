{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christine_Hsieh\\AppData\\Local\\Continuum\\anaconda3\\envs\\tome\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import tome\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model_name = \"vit_base_patch16_224\"\n",
    "model = timm.create_model(model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setting\n",
    "device = \"cuda:0\"\n",
    "runs = 50\n",
    "batch_size = 256  # Lower this if you don't have that much memory\n",
    "input_size = model.default_cfg[\"input_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|██████████| 50/50 [01:05<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 178.13 im/s\n"
     ]
    }
   ],
   "source": [
    "# Baseline benchmark\n",
    "baseline_throughput = tome.utils.benchmark(\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    runs=runs,\n",
    "    batch_size=batch_size,\n",
    "    input_size=input_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VisionTransformer                        [256, 1000]               152,064\n",
       "├─PatchEmbed: 1-1                        [256, 196, 768]           --\n",
       "│    └─Conv2d: 2-1                       [256, 768, 14, 14]        590,592\n",
       "│    └─Identity: 2-2                     [256, 196, 768]           --\n",
       "├─Dropout: 1-2                           [256, 197, 768]           --\n",
       "├─Sequential: 1-3                        [256, 197, 768]           --\n",
       "│    └─Block: 2-3                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-1               [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-2               [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-3                [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-4               [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-5                     [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-6                [256, 197, 768]           --\n",
       "│    └─Block: 2-4                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-7               [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-8               [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-9                [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-10              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-11                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-12               [256, 197, 768]           --\n",
       "│    └─Block: 2-5                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-13              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-14              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-15               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-16              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-17                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-18               [256, 197, 768]           --\n",
       "│    └─Block: 2-6                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-19              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-20              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-21               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-22              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-23                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-24               [256, 197, 768]           --\n",
       "│    └─Block: 2-7                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-25              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-26              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-27               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-28              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-29                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-30               [256, 197, 768]           --\n",
       "│    └─Block: 2-8                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-31              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-32              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-33               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-34              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-35                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-36               [256, 197, 768]           --\n",
       "│    └─Block: 2-9                        [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-37              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-38              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-39               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-40              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-41                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-42               [256, 197, 768]           --\n",
       "│    └─Block: 2-10                       [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-43              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-44              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-45               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-46              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-47                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-48               [256, 197, 768]           --\n",
       "│    └─Block: 2-11                       [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-49              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-50              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-51               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-52              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-53                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-54               [256, 197, 768]           --\n",
       "│    └─Block: 2-12                       [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-55              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-56              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-57               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-58              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-59                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-60               [256, 197, 768]           --\n",
       "│    └─Block: 2-13                       [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-61              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-62              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-63               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-64              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-65                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-66               [256, 197, 768]           --\n",
       "│    └─Block: 2-14                       [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-67              [256, 197, 768]           1,536\n",
       "│    │    └─Attention: 3-68              [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-69               [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-70              [256, 197, 768]           1,536\n",
       "│    │    └─Mlp: 3-71                    [256, 197, 768]           4,722,432\n",
       "│    │    └─Identity: 3-72               [256, 197, 768]           --\n",
       "├─LayerNorm: 1-4                         [256, 197, 768]           1,536\n",
       "├─Identity: 1-5                          [256, 768]                --\n",
       "├─Linear: 1-6                            [256, 1000]               769,000\n",
       "==========================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 51.60\n",
       "==========================================================================================\n",
       "Input size (MB): 154.14\n",
       "Forward/backward pass size (MB): 41520.94\n",
       "Params size (MB): 345.66\n",
       "Estimated Total Size (MB): 42020.74\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Apply ToMe \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|██████████| 50/50 [00:39<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 320.44 im/s\n",
      "Throughput improvement: 1.80x\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Apply ToMe \\n\")\n",
    "tome.patch.timm(model)\n",
    "# ToMe with r=16\n",
    "model.r = 16\n",
    "tome_throughput = tome.utils.benchmark(\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    runs=runs,\n",
    "    batch_size=batch_size,\n",
    "    input_size=input_size\n",
    ")\n",
    "print(f\"Throughput improvement: {tome_throughput / baseline_throughput:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ToMeVisionTransformer                    [256, 1000]               152,064\n",
       "├─PatchEmbed: 1-1                        [256, 196, 768]           --\n",
       "│    └─Conv2d: 2-1                       [256, 768, 14, 14]        590,592\n",
       "│    └─Identity: 2-2                     [256, 196, 768]           --\n",
       "├─Dropout: 1-2                           [256, 197, 768]           --\n",
       "├─Sequential: 1-3                        [256, 11, 768]            --\n",
       "│    └─ToMeBlock: 2-3                    [256, 181, 768]           --\n",
       "│    │    └─LayerNorm: 3-1               [256, 197, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-2           [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-3                [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-4               [256, 181, 768]           1,536\n",
       "│    │    └─Mlp: 3-5                     [256, 181, 768]           4,722,432\n",
       "│    │    └─Identity: 3-6                [256, 181, 768]           --\n",
       "│    └─ToMeBlock: 2-4                    [256, 165, 768]           --\n",
       "│    │    └─LayerNorm: 3-7               [256, 181, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-8           [256, 181, 768]           2,362,368\n",
       "│    │    └─Identity: 3-9                [256, 181, 768]           --\n",
       "│    │    └─LayerNorm: 3-10              [256, 165, 768]           1,536\n",
       "│    │    └─Mlp: 3-11                    [256, 165, 768]           4,722,432\n",
       "│    │    └─Identity: 3-12               [256, 165, 768]           --\n",
       "│    └─ToMeBlock: 2-5                    [256, 149, 768]           --\n",
       "│    │    └─LayerNorm: 3-13              [256, 165, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-14          [256, 165, 768]           2,362,368\n",
       "│    │    └─Identity: 3-15               [256, 165, 768]           --\n",
       "│    │    └─LayerNorm: 3-16              [256, 149, 768]           1,536\n",
       "│    │    └─Mlp: 3-17                    [256, 149, 768]           4,722,432\n",
       "│    │    └─Identity: 3-18               [256, 149, 768]           --\n",
       "│    └─ToMeBlock: 2-6                    [256, 133, 768]           --\n",
       "│    │    └─LayerNorm: 3-19              [256, 149, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-20          [256, 149, 768]           2,362,368\n",
       "│    │    └─Identity: 3-21               [256, 149, 768]           --\n",
       "│    │    └─LayerNorm: 3-22              [256, 133, 768]           1,536\n",
       "│    │    └─Mlp: 3-23                    [256, 133, 768]           4,722,432\n",
       "│    │    └─Identity: 3-24               [256, 133, 768]           --\n",
       "│    └─ToMeBlock: 2-7                    [256, 117, 768]           --\n",
       "│    │    └─LayerNorm: 3-25              [256, 133, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-26          [256, 133, 768]           2,362,368\n",
       "│    │    └─Identity: 3-27               [256, 133, 768]           --\n",
       "│    │    └─LayerNorm: 3-28              [256, 117, 768]           1,536\n",
       "│    │    └─Mlp: 3-29                    [256, 117, 768]           4,722,432\n",
       "│    │    └─Identity: 3-30               [256, 117, 768]           --\n",
       "│    └─ToMeBlock: 2-8                    [256, 101, 768]           --\n",
       "│    │    └─LayerNorm: 3-31              [256, 117, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-32          [256, 117, 768]           2,362,368\n",
       "│    │    └─Identity: 3-33               [256, 117, 768]           --\n",
       "│    │    └─LayerNorm: 3-34              [256, 101, 768]           1,536\n",
       "│    │    └─Mlp: 3-35                    [256, 101, 768]           4,722,432\n",
       "│    │    └─Identity: 3-36               [256, 101, 768]           --\n",
       "│    └─ToMeBlock: 2-9                    [256, 85, 768]            --\n",
       "│    │    └─LayerNorm: 3-37              [256, 101, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-38          [256, 101, 768]           2,362,368\n",
       "│    │    └─Identity: 3-39               [256, 101, 768]           --\n",
       "│    │    └─LayerNorm: 3-40              [256, 85, 768]            1,536\n",
       "│    │    └─Mlp: 3-41                    [256, 85, 768]            4,722,432\n",
       "│    │    └─Identity: 3-42               [256, 85, 768]            --\n",
       "│    └─ToMeBlock: 2-10                   [256, 69, 768]            --\n",
       "│    │    └─LayerNorm: 3-43              [256, 85, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-44          [256, 85, 768]            2,362,368\n",
       "│    │    └─Identity: 3-45               [256, 85, 768]            --\n",
       "│    │    └─LayerNorm: 3-46              [256, 69, 768]            1,536\n",
       "│    │    └─Mlp: 3-47                    [256, 69, 768]            4,722,432\n",
       "│    │    └─Identity: 3-48               [256, 69, 768]            --\n",
       "│    └─ToMeBlock: 2-11                   [256, 53, 768]            --\n",
       "│    │    └─LayerNorm: 3-49              [256, 69, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-50          [256, 69, 768]            2,362,368\n",
       "│    │    └─Identity: 3-51               [256, 69, 768]            --\n",
       "│    │    └─LayerNorm: 3-52              [256, 53, 768]            1,536\n",
       "│    │    └─Mlp: 3-53                    [256, 53, 768]            4,722,432\n",
       "│    │    └─Identity: 3-54               [256, 53, 768]            --\n",
       "│    └─ToMeBlock: 2-12                   [256, 37, 768]            --\n",
       "│    │    └─LayerNorm: 3-55              [256, 53, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-56          [256, 53, 768]            2,362,368\n",
       "│    │    └─Identity: 3-57               [256, 53, 768]            --\n",
       "│    │    └─LayerNorm: 3-58              [256, 37, 768]            1,536\n",
       "│    │    └─Mlp: 3-59                    [256, 37, 768]            4,722,432\n",
       "│    │    └─Identity: 3-60               [256, 37, 768]            --\n",
       "│    └─ToMeBlock: 2-13                   [256, 21, 768]            --\n",
       "│    │    └─LayerNorm: 3-61              [256, 37, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-62          [256, 37, 768]            2,362,368\n",
       "│    │    └─Identity: 3-63               [256, 37, 768]            --\n",
       "│    │    └─LayerNorm: 3-64              [256, 21, 768]            1,536\n",
       "│    │    └─Mlp: 3-65                    [256, 21, 768]            4,722,432\n",
       "│    │    └─Identity: 3-66               [256, 21, 768]            --\n",
       "│    └─ToMeBlock: 2-14                   [256, 11, 768]            --\n",
       "│    │    └─LayerNorm: 3-67              [256, 21, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-68          [256, 21, 768]            2,362,368\n",
       "│    │    └─Identity: 3-69               [256, 21, 768]            --\n",
       "│    │    └─LayerNorm: 3-70              [256, 11, 768]            1,536\n",
       "│    │    └─Mlp: 3-71                    [256, 11, 768]            4,722,432\n",
       "│    │    └─Identity: 3-72               [256, 11, 768]            --\n",
       "├─LayerNorm: 1-4                         [256, 11, 768]            1,536\n",
       "├─Identity: 1-5                          [256, 768]                --\n",
       "├─Linear: 1-6                            [256, 1000]               769,000\n",
       "==========================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 51.60\n",
       "==========================================================================================\n",
       "Input size (MB): 154.14\n",
       "Forward/backward pass size (MB): 21202.68\n",
       "Params size (MB): 345.66\n",
       "Estimated Total Size (MB): 21702.48\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Apply ToMe with decreasing schedule \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|██████████| 50/50 [00:26<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 469.72 im/s\n",
      "Throughput improvement: 2.64x\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Apply ToMe with decreasing schedule \\n\")\n",
    "model.r = (16, -1.0)\n",
    "tome_decr_throughput = tome.utils.benchmark(\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    runs=runs,\n",
    "    batch_size=batch_size,\n",
    "    input_size=input_size\n",
    ")\n",
    "print(f\"Throughput improvement: {tome_decr_throughput / baseline_throughput:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ToMeVisionTransformer                    [256, 1000]               152,064\n",
       "├─PatchEmbed: 1-1                        [256, 196, 768]           --\n",
       "│    └─Conv2d: 2-1                       [256, 768, 14, 14]        590,592\n",
       "│    └─Identity: 2-2                     [256, 196, 768]           --\n",
       "├─Dropout: 1-2                           [256, 197, 768]           --\n",
       "├─Sequential: 1-3                        [256, 10, 768]            --\n",
       "│    └─ToMeBlock: 2-3                    [256, 165, 768]           --\n",
       "│    │    └─LayerNorm: 3-1               [256, 197, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-2           [256, 197, 768]           2,362,368\n",
       "│    │    └─Identity: 3-3                [256, 197, 768]           --\n",
       "│    │    └─LayerNorm: 3-4               [256, 165, 768]           1,536\n",
       "│    │    └─Mlp: 3-5                     [256, 165, 768]           4,722,432\n",
       "│    │    └─Identity: 3-6                [256, 165, 768]           --\n",
       "│    └─ToMeBlock: 2-4                    [256, 136, 768]           --\n",
       "│    │    └─LayerNorm: 3-7               [256, 165, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-8           [256, 165, 768]           2,362,368\n",
       "│    │    └─Identity: 3-9                [256, 165, 768]           --\n",
       "│    │    └─LayerNorm: 3-10              [256, 136, 768]           1,536\n",
       "│    │    └─Mlp: 3-11                    [256, 136, 768]           4,722,432\n",
       "│    │    └─Identity: 3-12               [256, 136, 768]           --\n",
       "│    └─ToMeBlock: 2-5                    [256, 110, 768]           --\n",
       "│    │    └─LayerNorm: 3-13              [256, 136, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-14          [256, 136, 768]           2,362,368\n",
       "│    │    └─Identity: 3-15               [256, 136, 768]           --\n",
       "│    │    └─LayerNorm: 3-16              [256, 110, 768]           1,536\n",
       "│    │    └─Mlp: 3-17                    [256, 110, 768]           4,722,432\n",
       "│    │    └─Identity: 3-18               [256, 110, 768]           --\n",
       "│    └─ToMeBlock: 2-6                    [256, 87, 768]            --\n",
       "│    │    └─LayerNorm: 3-19              [256, 110, 768]           1,536\n",
       "│    │    └─ToMeAttention: 3-20          [256, 110, 768]           2,362,368\n",
       "│    │    └─Identity: 3-21               [256, 110, 768]           --\n",
       "│    │    └─LayerNorm: 3-22              [256, 87, 768]            1,536\n",
       "│    │    └─Mlp: 3-23                    [256, 87, 768]            4,722,432\n",
       "│    │    └─Identity: 3-24               [256, 87, 768]            --\n",
       "│    └─ToMeBlock: 2-7                    [256, 67, 768]            --\n",
       "│    │    └─LayerNorm: 3-25              [256, 87, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-26          [256, 87, 768]            2,362,368\n",
       "│    │    └─Identity: 3-27               [256, 87, 768]            --\n",
       "│    │    └─LayerNorm: 3-28              [256, 67, 768]            1,536\n",
       "│    │    └─Mlp: 3-29                    [256, 67, 768]            4,722,432\n",
       "│    │    └─Identity: 3-30               [256, 67, 768]            --\n",
       "│    └─ToMeBlock: 2-8                    [256, 50, 768]            --\n",
       "│    │    └─LayerNorm: 3-31              [256, 67, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-32          [256, 67, 768]            2,362,368\n",
       "│    │    └─Identity: 3-33               [256, 67, 768]            --\n",
       "│    │    └─LayerNorm: 3-34              [256, 50, 768]            1,536\n",
       "│    │    └─Mlp: 3-35                    [256, 50, 768]            4,722,432\n",
       "│    │    └─Identity: 3-36               [256, 50, 768]            --\n",
       "│    └─ToMeBlock: 2-9                    [256, 36, 768]            --\n",
       "│    │    └─LayerNorm: 3-37              [256, 50, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-38          [256, 50, 768]            2,362,368\n",
       "│    │    └─Identity: 3-39               [256, 50, 768]            --\n",
       "│    │    └─LayerNorm: 3-40              [256, 36, 768]            1,536\n",
       "│    │    └─Mlp: 3-41                    [256, 36, 768]            4,722,432\n",
       "│    │    └─Identity: 3-42               [256, 36, 768]            --\n",
       "│    └─ToMeBlock: 2-10                   [256, 25, 768]            --\n",
       "│    │    └─LayerNorm: 3-43              [256, 36, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-44          [256, 36, 768]            2,362,368\n",
       "│    │    └─Identity: 3-45               [256, 36, 768]            --\n",
       "│    │    └─LayerNorm: 3-46              [256, 25, 768]            1,536\n",
       "│    │    └─Mlp: 3-47                    [256, 25, 768]            4,722,432\n",
       "│    │    └─Identity: 3-48               [256, 25, 768]            --\n",
       "│    └─ToMeBlock: 2-11                   [256, 17, 768]            --\n",
       "│    │    └─LayerNorm: 3-49              [256, 25, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-50          [256, 25, 768]            2,362,368\n",
       "│    │    └─Identity: 3-51               [256, 25, 768]            --\n",
       "│    │    └─LayerNorm: 3-52              [256, 17, 768]            1,536\n",
       "│    │    └─Mlp: 3-53                    [256, 17, 768]            4,722,432\n",
       "│    │    └─Identity: 3-54               [256, 17, 768]            --\n",
       "│    └─ToMeBlock: 2-12                   [256, 12, 768]            --\n",
       "│    │    └─LayerNorm: 3-55              [256, 17, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-56          [256, 17, 768]            2,362,368\n",
       "│    │    └─Identity: 3-57               [256, 17, 768]            --\n",
       "│    │    └─LayerNorm: 3-58              [256, 12, 768]            1,536\n",
       "│    │    └─Mlp: 3-59                    [256, 12, 768]            4,722,432\n",
       "│    │    └─Identity: 3-60               [256, 12, 768]            --\n",
       "│    └─ToMeBlock: 2-13                   [256, 10, 768]            --\n",
       "│    │    └─LayerNorm: 3-61              [256, 12, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-62          [256, 12, 768]            2,362,368\n",
       "│    │    └─Identity: 3-63               [256, 12, 768]            --\n",
       "│    │    └─LayerNorm: 3-64              [256, 10, 768]            1,536\n",
       "│    │    └─Mlp: 3-65                    [256, 10, 768]            4,722,432\n",
       "│    │    └─Identity: 3-66               [256, 10, 768]            --\n",
       "│    └─ToMeBlock: 2-14                   [256, 10, 768]            --\n",
       "│    │    └─LayerNorm: 3-67              [256, 10, 768]            1,536\n",
       "│    │    └─ToMeAttention: 3-68          [256, 10, 768]            2,362,368\n",
       "│    │    └─Identity: 3-69               [256, 10, 768]            --\n",
       "│    │    └─LayerNorm: 3-70              [256, 10, 768]            1,536\n",
       "│    │    └─Mlp: 3-71                    [256, 10, 768]            4,722,432\n",
       "│    │    └─Identity: 3-72               [256, 10, 768]            --\n",
       "├─LayerNorm: 1-4                         [256, 10, 768]            1,536\n",
       "├─Identity: 1-5                          [256, 768]                --\n",
       "├─Linear: 1-6                            [256, 1000]               769,000\n",
       "==========================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 51.60\n",
       "==========================================================================================\n",
       "Input size (MB): 154.14\n",
       "Forward/backward pass size (MB): 14340.28\n",
       "Params size (MB): 345.66\n",
       "Estimated Total Size (MB): 14840.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_mobilenetv3_large_075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|██████████| 50/50 [00:06<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 1869.02 im/s\n"
     ]
    }
   ],
   "source": [
    "# Load mobileNet\n",
    "model_name = \"tf_mobilenetv3_large_075\"\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "device = \"cuda:0\"\n",
    "runs = 50\n",
    "batch_size = 256  # Lower this if you don't have that much memory\n",
    "input_size = model.default_cfg[\"input_size\"]\n",
    "print(model_name)\n",
    "baseline_throughput = tome.utils.benchmark(\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    runs=runs,\n",
    "    batch_size=batch_size,\n",
    "    input_size=input_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "MobileNetV3                                   [256, 1000]               --\n",
       "├─Conv2dSame: 1-1                             [256, 16, 112, 112]       432\n",
       "├─BatchNorm2d: 1-2                            [256, 16, 112, 112]       32\n",
       "├─Hardswish: 1-3                              [256, 16, 112, 112]       --\n",
       "├─Sequential: 1-4                             [256, 720, 7, 7]          --\n",
       "│    └─Sequential: 2-1                        [256, 16, 112, 112]       --\n",
       "│    │    └─DepthwiseSeparableConv: 3-1       [256, 16, 112, 112]       464\n",
       "│    └─Sequential: 2-2                        [256, 24, 56, 56]         --\n",
       "│    │    └─InvertedResidual: 3-2             [256, 24, 56, 56]         3,440\n",
       "│    │    └─InvertedResidual: 3-3             [256, 24, 56, 56]         4,440\n",
       "│    └─Sequential: 2-3                        [256, 32, 28, 28]         --\n",
       "│    │    └─InvertedResidual: 3-4             [256, 32, 28, 28]         9,736\n",
       "│    │    └─InvertedResidual: 3-5             [256, 32, 28, 28]         13,720\n",
       "│    │    └─InvertedResidual: 3-6             [256, 32, 28, 28]         13,720\n",
       "│    └─Sequential: 2-4                        [256, 64, 14, 14]         --\n",
       "│    │    └─InvertedResidual: 3-7             [256, 64, 14, 14]         21,056\n",
       "│    │    └─InvertedResidual: 3-8             [256, 64, 14, 14]         22,688\n",
       "│    │    └─InvertedResidual: 3-9             [256, 64, 14, 14]         20,432\n",
       "│    │    └─InvertedResidual: 3-10            [256, 64, 14, 14]         20,432\n",
       "│    └─Sequential: 2-5                        [256, 88, 14, 14]         --\n",
       "│    │    └─InvertedResidual: 3-11            [256, 88, 14, 14]         137,744\n",
       "│    │    └─InvertedResidual: 3-12            [256, 88, 14, 14]         244,248\n",
       "│    └─Sequential: 2-6                        [256, 120, 7, 7]          --\n",
       "│    │    └─InvertedResidual: 3-13            [256, 120, 7, 7]          269,656\n",
       "│    │    └─InvertedResidual: 3-14            [256, 120, 7, 7]          459,784\n",
       "│    │    └─InvertedResidual: 3-15            [256, 120, 7, 7]          459,784\n",
       "│    └─Sequential: 2-7                        [256, 720, 7, 7]          --\n",
       "│    │    └─ConvBnAct: 3-16                   [256, 720, 7, 7]          87,840\n",
       "├─SelectAdaptivePool2d: 1-5                   [256, 720, 1, 1]          --\n",
       "│    └─AdaptiveAvgPool2d: 2-8                 [256, 720, 1, 1]          --\n",
       "│    └─Identity: 2-9                          [256, 720, 1, 1]          --\n",
       "├─Conv2d: 1-6                                 [256, 1280, 1, 1]         922,880\n",
       "├─Hardswish: 1-7                              [256, 1280, 1, 1]         --\n",
       "├─Flatten: 1-8                                [256, 1280]               --\n",
       "├─Linear: 1-9                                 [256, 1000]               1,281,000\n",
       "===============================================================================================\n",
       "Total params: 3,993,528\n",
       "Trainable params: 3,993,528\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 39.57\n",
       "===============================================================================================\n",
       "Input size (MB): 154.14\n",
       "Forward/backward pass size (MB): 16378.97\n",
       "Params size (MB): 15.97\n",
       "Estimated Total Size (MB): 16549.08\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenetv4_hybrid_medium_075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\Christine_Hsieh\\AppData\\Local\\Continuum\\anaconda3\\envs\\tome\\lib\\site-packages\\timm\\layers\\attention2d.py:273: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  o = F.scaled_dot_product_attention(\n",
      "Benchmarking: 100%|██████████| 50/50 [00:07<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 1590.61 im/s\n"
     ]
    }
   ],
   "source": [
    "# Load mobileNet\n",
    "model_name = \"mobilenetv4_hybrid_medium_075\"\n",
    "model = timm.create_model(model_name, pretrained=False)\n",
    "device = \"cuda:0\"\n",
    "runs = 50\n",
    "batch_size = 256  # Lower this if you don't have that much memory\n",
    "input_size = model.default_cfg[\"input_size\"]\n",
    "print(model_name)\n",
    "baseline_throughput = tome.utils.benchmark(\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    runs=runs,\n",
    "    batch_size=batch_size,\n",
    "    input_size=input_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "MobileNetV3                                        [256, 1000]               --\n",
       "├─Conv2d: 1-1                                      [256, 32, 112, 112]       864\n",
       "├─BatchNormAct2d: 1-2                              [256, 32, 112, 112]       64\n",
       "│    └─Identity: 2-1                               [256, 32, 112, 112]       --\n",
       "│    └─ReLU: 2-2                                   [256, 32, 112, 112]       --\n",
       "├─Sequential: 1-3                                  [256, 720, 7, 7]          --\n",
       "│    └─Sequential: 2-3                             [256, 40, 56, 56]         --\n",
       "│    │    └─EdgeResidual: 3-1                      [256, 40, 56, 56]         42,320\n",
       "│    └─Sequential: 2-4                             [256, 64, 28, 28]         --\n",
       "│    │    └─UniversalInvertedResidual: 3-2         [256, 64, 28, 28]         21,912\n",
       "│    │    └─UniversalInvertedResidual: 3-3         [256, 64, 28, 28]         18,944\n",
       "│    └─Sequential: 2-5                             [256, 120, 14, 14]        --\n",
       "│    │    └─UniversalInvertedResidual: 3-4         [256, 120, 14, 14]        82,856\n",
       "│    │    └─UniversalInvertedResidual: 3-5         [256, 120, 14, 14]        58,440\n",
       "│    │    └─UniversalInvertedResidual: 3-6         [256, 120, 14, 14]        123,120\n",
       "│    │    └─UniversalInvertedResidual: 3-7         [256, 120, 14, 14]        130,800\n",
       "│    │    └─MobileAttention: 3-8                   [256, 120, 14, 14]        79,800\n",
       "│    │    └─UniversalInvertedResidual: 3-9         [256, 120, 14, 14]        123,120\n",
       "│    │    └─MobileAttention: 3-10                  [256, 120, 14, 14]        79,800\n",
       "│    │    └─UniversalInvertedResidual: 3-11        [256, 120, 14, 14]        117,840\n",
       "│    │    └─MobileAttention: 3-12                  [256, 120, 14, 14]        79,800\n",
       "│    │    └─UniversalInvertedResidual: 3-13        [256, 120, 14, 14]        123,120\n",
       "│    │    └─MobileAttention: 3-14                  [256, 120, 14, 14]        79,800\n",
       "│    │    └─UniversalInvertedResidual: 3-15        [256, 120, 14, 14]        117,840\n",
       "│    └─Sequential: 2-6                             [256, 192, 7, 7]          --\n",
       "│    │    └─UniversalInvertedResidual: 3-16        [256, 192, 7, 7]          249,336\n",
       "│    │    └─UniversalInvertedResidual: 3-17        [256, 192, 7, 7]          322,944\n",
       "│    │    └─UniversalInvertedResidual: 3-18        [256, 192, 7, 7]          319,872\n",
       "│    │    └─UniversalInvertedResidual: 3-19        [256, 192, 7, 7]          319,872\n",
       "│    │    └─UniversalInvertedResidual: 3-20        [256, 192, 7, 7]          148,800\n",
       "│    │    └─UniversalInvertedResidual: 3-21        [256, 192, 7, 7]          161,280\n",
       "│    │    └─UniversalInvertedResidual: 3-22        [256, 192, 7, 7]          148,800\n",
       "│    │    └─UniversalInvertedResidual: 3-23        [256, 192, 7, 7]          297,024\n",
       "│    │    └─MobileAttention: 3-24                  [256, 192, 7, 7]          123,456\n",
       "│    │    └─UniversalInvertedResidual: 3-25        [256, 192, 7, 7]          299,136\n",
       "│    │    └─MobileAttention: 3-26                  [256, 192, 7, 7]          123,456\n",
       "│    │    └─UniversalInvertedResidual: 3-27        [256, 192, 7, 7]          322,944\n",
       "│    │    └─MobileAttention: 3-28                  [256, 192, 7, 7]          123,456\n",
       "│    │    └─UniversalInvertedResidual: 3-29        [256, 192, 7, 7]          302,208\n",
       "│    │    └─MobileAttention: 3-30                  [256, 192, 7, 7]          123,456\n",
       "│    │    └─UniversalInvertedResidual: 3-31        [256, 192, 7, 7]          302,208\n",
       "│    └─Sequential: 2-7                             [256, 720, 7, 7]          --\n",
       "│    │    └─ConvBnAct: 3-32                        [256, 720, 7, 7]          139,680\n",
       "├─SelectAdaptivePool2d: 1-4                        [256, 720, 1, 1]          --\n",
       "│    └─AdaptiveAvgPool2d: 2-8                      [256, 720, 1, 1]          --\n",
       "│    └─Identity: 2-9                               [256, 720, 1, 1]          --\n",
       "├─Conv2d: 1-5                                      [256, 1280, 1, 1]         921,600\n",
       "├─BatchNormAct2d: 1-6                              [256, 1280, 1, 1]         2,560\n",
       "│    └─Identity: 2-10                              [256, 1280, 1, 1]         --\n",
       "│    └─ReLU: 2-11                                  [256, 1280, 1, 1]         --\n",
       "├─Identity: 1-7                                    [256, 1280, 1, 1]         --\n",
       "├─Flatten: 1-8                                     [256, 1280]               --\n",
       "├─Linear: 1-9                                      [256, 1000]               1,281,000\n",
       "====================================================================================================\n",
       "Total params: 7,313,528\n",
       "Trainable params: 7,313,528\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 160.07\n",
       "====================================================================================================\n",
       "Input size (MB): 154.14\n",
       "Forward/backward pass size (MB): 11854.23\n",
       "Params size (MB): 29.03\n",
       "Estimated Total Size (MB): 12037.40\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adv_inception_v3\n",
      "bat_resnext26ts\n",
      "botnet26t_256\n",
      "botnet50ts_256\n",
      "cait_m36_384\n",
      "cait_m48_448\n",
      "cait_s24_224\n",
      "cait_s24_384\n",
      "cait_s36_384\n",
      "cait_xs24_384\n",
      "cait_xxs24_224\n",
      "cait_xxs24_384\n",
      "cait_xxs36_224\n",
      "cait_xxs36_384\n",
      "coat_lite_mini\n",
      "coat_lite_small\n",
      "coat_lite_tiny\n",
      "coat_mini\n",
      "coat_tiny\n",
      "convit_base\n",
      "convit_small\n",
      "convit_tiny\n",
      "cspdarknet53\n",
      "cspdarknet53_iabn\n",
      "cspresnet50\n",
      "cspresnet50d\n",
      "cspresnet50w\n",
      "cspresnext50\n",
      "cspresnext50_iabn\n",
      "darknet53\n",
      "deit_base_distilled_patch16_224\n",
      "deit_base_distilled_patch16_384\n",
      "deit_base_patch16_224\n",
      "deit_base_patch16_384\n",
      "deit_small_distilled_patch16_224\n",
      "deit_small_patch16_224\n",
      "deit_tiny_distilled_patch16_224\n",
      "deit_tiny_patch16_224\n",
      "densenet121\n",
      "densenet121d\n",
      "densenet161\n",
      "densenet169\n",
      "densenet201\n",
      "densenet264\n",
      "densenet264d_iabn\n",
      "densenetblur121d\n",
      "dla34\n",
      "dla46_c\n",
      "dla46x_c\n",
      "dla60\n",
      "dla60_res2net\n",
      "dla60_res2next\n",
      "dla60x\n",
      "dla60x_c\n",
      "dla102\n",
      "dla102x\n",
      "dla102x2\n",
      "dla169\n",
      "dm_nfnet_f0\n",
      "dm_nfnet_f1\n",
      "dm_nfnet_f2\n",
      "dm_nfnet_f3\n",
      "dm_nfnet_f4\n",
      "dm_nfnet_f5\n",
      "dm_nfnet_f6\n",
      "dpn68\n",
      "dpn68b\n",
      "dpn92\n",
      "dpn98\n",
      "dpn107\n",
      "dpn131\n",
      "eca_botnext26ts_256\n",
      "eca_efficientnet_b0\n",
      "eca_halonext26ts\n",
      "eca_lambda_resnext26ts\n",
      "eca_nfnet_l0\n",
      "eca_nfnet_l1\n",
      "eca_nfnet_l2\n",
      "eca_nfnet_l3\n",
      "eca_swinnext26ts_256\n",
      "eca_vovnet39b\n",
      "ecaresnet26t\n",
      "ecaresnet50d\n",
      "ecaresnet50d_pruned\n",
      "ecaresnet50t\n",
      "ecaresnet101d\n",
      "ecaresnet101d_pruned\n",
      "ecaresnet200d\n",
      "ecaresnet269d\n",
      "ecaresnetlight\n",
      "ecaresnext26t_32x4d\n",
      "ecaresnext50t_32x4d\n",
      "efficientnet_b0\n",
      "efficientnet_b1\n",
      "efficientnet_b1_pruned\n",
      "efficientnet_b2\n",
      "efficientnet_b2_pruned\n",
      "efficientnet_b2a\n",
      "efficientnet_b3\n",
      "efficientnet_b3_pruned\n",
      "efficientnet_b3a\n",
      "efficientnet_b4\n",
      "efficientnet_b5\n",
      "efficientnet_b6\n",
      "efficientnet_b7\n",
      "efficientnet_b8\n",
      "efficientnet_cc_b0_4e\n",
      "efficientnet_cc_b0_8e\n",
      "efficientnet_cc_b1_8e\n",
      "efficientnet_el\n",
      "efficientnet_el_pruned\n",
      "efficientnet_em\n",
      "efficientnet_es\n",
      "efficientnet_es_pruned\n",
      "efficientnet_l2\n",
      "efficientnet_lite0\n",
      "efficientnet_lite1\n",
      "efficientnet_lite2\n",
      "efficientnet_lite3\n",
      "efficientnet_lite4\n",
      "efficientnetv2_l\n",
      "efficientnetv2_m\n",
      "efficientnetv2_rw_m\n",
      "efficientnetv2_rw_s\n",
      "efficientnetv2_s\n",
      "ens_adv_inception_resnet_v2\n",
      "ese_vovnet19b_dw\n",
      "ese_vovnet19b_slim\n",
      "ese_vovnet19b_slim_dw\n",
      "ese_vovnet39b\n",
      "ese_vovnet39b_evos\n",
      "ese_vovnet57b\n",
      "ese_vovnet99b\n",
      "ese_vovnet99b_iabn\n",
      "fbnetc_100\n",
      "fbnetv3_b\n",
      "fbnetv3_d\n",
      "fbnetv3_g\n",
      "gc_efficientnet_b0\n",
      "gcresnet50t\n",
      "gcresnext26ts\n",
      "geresnet50t\n",
      "gernet_l\n",
      "gernet_m\n",
      "gernet_s\n",
      "ghostnet_050\n",
      "ghostnet_100\n",
      "ghostnet_130\n",
      "gluon_inception_v3\n",
      "gluon_resnet18_v1b\n",
      "gluon_resnet34_v1b\n",
      "gluon_resnet50_v1b\n",
      "gluon_resnet50_v1c\n",
      "gluon_resnet50_v1d\n",
      "gluon_resnet50_v1s\n",
      "gluon_resnet101_v1b\n",
      "gluon_resnet101_v1c\n",
      "gluon_resnet101_v1d\n",
      "gluon_resnet101_v1s\n",
      "gluon_resnet152_v1b\n",
      "gluon_resnet152_v1c\n",
      "gluon_resnet152_v1d\n",
      "gluon_resnet152_v1s\n",
      "gluon_resnext50_32x4d\n",
      "gluon_resnext101_32x4d\n",
      "gluon_resnext101_64x4d\n",
      "gluon_senet154\n",
      "gluon_seresnext50_32x4d\n",
      "gluon_seresnext101_32x4d\n",
      "gluon_seresnext101_64x4d\n",
      "gluon_xception65\n",
      "gmixer_12_224\n",
      "gmixer_24_224\n",
      "gmlp_b16_224\n",
      "gmlp_s16_224\n",
      "gmlp_ti16_224\n",
      "halonet26t\n",
      "halonet50ts\n",
      "halonet_h1\n",
      "halonet_h1_c4c5\n",
      "hardcorenas_a\n",
      "hardcorenas_b\n",
      "hardcorenas_c\n",
      "hardcorenas_d\n",
      "hardcorenas_e\n",
      "hardcorenas_f\n",
      "hrnet_w18\n",
      "hrnet_w18_small\n",
      "hrnet_w18_small_v2\n",
      "hrnet_w30\n",
      "hrnet_w32\n",
      "hrnet_w40\n",
      "hrnet_w44\n",
      "hrnet_w48\n",
      "hrnet_w64\n",
      "ig_resnext101_32x8d\n",
      "ig_resnext101_32x16d\n",
      "ig_resnext101_32x32d\n",
      "ig_resnext101_32x48d\n",
      "inception_resnet_v2\n",
      "inception_v3\n",
      "inception_v4\n",
      "lambda_resnet26t\n",
      "lambda_resnet50t\n",
      "legacy_senet154\n",
      "legacy_seresnet18\n",
      "legacy_seresnet34\n",
      "legacy_seresnet50\n",
      "legacy_seresnet101\n",
      "legacy_seresnet152\n",
      "legacy_seresnext26_32x4d\n",
      "legacy_seresnext50_32x4d\n",
      "legacy_seresnext101_32x4d\n",
      "levit_128\n",
      "levit_128s\n",
      "levit_192\n",
      "levit_256\n",
      "levit_384\n",
      "mixer_b16_224\n",
      "mixer_b16_224_in21k\n",
      "mixer_b16_224_miil\n",
      "mixer_b16_224_miil_in21k\n",
      "mixer_b32_224\n",
      "mixer_l16_224\n",
      "mixer_l16_224_in21k\n",
      "mixer_l32_224\n",
      "mixer_s16_224\n",
      "mixer_s32_224\n",
      "mixnet_l\n",
      "mixnet_m\n",
      "mixnet_s\n",
      "mixnet_xl\n",
      "mixnet_xxl\n",
      "mnasnet_050\n",
      "mnasnet_075\n",
      "mnasnet_100\n",
      "mnasnet_140\n",
      "mnasnet_a1\n",
      "mnasnet_b1\n",
      "mnasnet_small\n",
      "mobilenetv2_100\n",
      "mobilenetv2_110d\n",
      "mobilenetv2_120d\n",
      "mobilenetv2_140\n",
      "mobilenetv3_large_075\n",
      "mobilenetv3_large_100\n",
      "mobilenetv3_large_100_miil\n",
      "mobilenetv3_large_100_miil_in21k\n",
      "mobilenetv3_rw\n",
      "mobilenetv3_small_075\n",
      "mobilenetv3_small_100\n",
      "nasnetalarge\n",
      "nf_ecaresnet26\n",
      "nf_ecaresnet50\n",
      "nf_ecaresnet101\n",
      "nf_regnet_b0\n",
      "nf_regnet_b1\n",
      "nf_regnet_b2\n",
      "nf_regnet_b3\n",
      "nf_regnet_b4\n",
      "nf_regnet_b5\n",
      "nf_resnet26\n",
      "nf_resnet50\n",
      "nf_resnet101\n",
      "nf_seresnet26\n",
      "nf_seresnet50\n",
      "nf_seresnet101\n",
      "nfnet_f0\n",
      "nfnet_f0s\n",
      "nfnet_f1\n",
      "nfnet_f1s\n",
      "nfnet_f2\n",
      "nfnet_f2s\n",
      "nfnet_f3\n",
      "nfnet_f3s\n",
      "nfnet_f4\n",
      "nfnet_f4s\n",
      "nfnet_f5\n",
      "nfnet_f5s\n",
      "nfnet_f6\n",
      "nfnet_f6s\n",
      "nfnet_f7\n",
      "nfnet_f7s\n",
      "nfnet_l0\n",
      "pit_b_224\n",
      "pit_b_distilled_224\n",
      "pit_s_224\n",
      "pit_s_distilled_224\n",
      "pit_ti_224\n",
      "pit_ti_distilled_224\n",
      "pit_xs_224\n",
      "pit_xs_distilled_224\n",
      "pnasnet5large\n",
      "rednet26t\n",
      "rednet50ts\n",
      "regnetx_002\n",
      "regnetx_004\n",
      "regnetx_006\n",
      "regnetx_008\n",
      "regnetx_016\n",
      "regnetx_032\n",
      "regnetx_040\n",
      "regnetx_064\n",
      "regnetx_080\n",
      "regnetx_120\n",
      "regnetx_160\n",
      "regnetx_320\n",
      "regnety_002\n",
      "regnety_004\n",
      "regnety_006\n",
      "regnety_008\n",
      "regnety_016\n",
      "regnety_032\n",
      "regnety_040\n",
      "regnety_064\n",
      "regnety_080\n",
      "regnety_120\n",
      "regnety_160\n",
      "regnety_320\n",
      "repvgg_a2\n",
      "repvgg_b0\n",
      "repvgg_b1\n",
      "repvgg_b1g4\n",
      "repvgg_b2\n",
      "repvgg_b2g4\n",
      "repvgg_b3\n",
      "repvgg_b3g4\n",
      "res2net50_14w_8s\n",
      "res2net50_26w_4s\n",
      "res2net50_26w_6s\n",
      "res2net50_26w_8s\n",
      "res2net50_48w_2s\n",
      "res2net101_26w_4s\n",
      "res2next50\n",
      "resmlp_12_224\n",
      "resmlp_12_distilled_224\n",
      "resmlp_24_224\n",
      "resmlp_24_distilled_224\n",
      "resmlp_36_224\n",
      "resmlp_36_distilled_224\n",
      "resmlp_big_24_224\n",
      "resmlp_big_24_224_in22ft1k\n",
      "resmlp_big_24_distilled_224\n",
      "resnest14d\n",
      "resnest26d\n",
      "resnest50d\n",
      "resnest50d_1s4x24d\n",
      "resnest50d_4s2x40d\n",
      "resnest101e\n",
      "resnest200e\n",
      "resnest269e\n",
      "resnet18\n",
      "resnet18d\n",
      "resnet26\n",
      "resnet26d\n",
      "resnet26t\n",
      "resnet34\n",
      "resnet34d\n",
      "resnet50\n",
      "resnet50d\n",
      "resnet50t\n",
      "resnet51q\n",
      "resnet61q\n",
      "resnet101\n",
      "resnet101d\n",
      "resnet152\n",
      "resnet152d\n",
      "resnet200\n",
      "resnet200d\n",
      "resnetblur18\n",
      "resnetblur50\n",
      "resnetrs50\n",
      "resnetrs101\n",
      "resnetrs152\n",
      "resnetrs200\n",
      "resnetrs270\n",
      "resnetrs350\n",
      "resnetrs420\n",
      "resnetv2_50\n",
      "resnetv2_50d\n",
      "resnetv2_50t\n",
      "resnetv2_50x1_bit_distilled\n",
      "resnetv2_50x1_bitm\n",
      "resnetv2_50x1_bitm_in21k\n",
      "resnetv2_50x3_bitm\n",
      "resnetv2_50x3_bitm_in21k\n",
      "resnetv2_101\n",
      "resnetv2_101d\n",
      "resnetv2_101x1_bitm\n",
      "resnetv2_101x1_bitm_in21k\n",
      "resnetv2_101x3_bitm\n",
      "resnetv2_101x3_bitm_in21k\n",
      "resnetv2_152\n",
      "resnetv2_152d\n",
      "resnetv2_152x2_bit_teacher\n",
      "resnetv2_152x2_bit_teacher_384\n",
      "resnetv2_152x2_bitm\n",
      "resnetv2_152x2_bitm_in21k\n",
      "resnetv2_152x4_bitm\n",
      "resnetv2_152x4_bitm_in21k\n",
      "resnext50_32x4d\n",
      "resnext50d_32x4d\n",
      "resnext101_32x4d\n",
      "resnext101_32x8d\n",
      "resnext101_64x4d\n",
      "rexnet_100\n",
      "rexnet_130\n",
      "rexnet_150\n",
      "rexnet_200\n",
      "rexnetr_100\n",
      "rexnetr_130\n",
      "rexnetr_150\n",
      "rexnetr_200\n",
      "selecsls42\n",
      "selecsls42b\n",
      "selecsls60\n",
      "selecsls60b\n",
      "selecsls84\n",
      "semnasnet_050\n",
      "semnasnet_075\n",
      "semnasnet_100\n",
      "semnasnet_140\n",
      "senet154\n",
      "seresnet18\n",
      "seresnet34\n",
      "seresnet50\n",
      "seresnet50t\n",
      "seresnet101\n",
      "seresnet152\n",
      "seresnet152d\n",
      "seresnet200d\n",
      "seresnet269d\n",
      "seresnext26d_32x4d\n",
      "seresnext26t_32x4d\n",
      "seresnext26tn_32x4d\n",
      "seresnext50_32x4d\n",
      "seresnext101_32x4d\n",
      "seresnext101_32x8d\n",
      "skresnet18\n",
      "skresnet34\n",
      "skresnet50\n",
      "skresnet50d\n",
      "skresnext50_32x4d\n",
      "spnasnet_100\n",
      "ssl_resnet18\n",
      "ssl_resnet50\n",
      "ssl_resnext50_32x4d\n",
      "ssl_resnext101_32x4d\n",
      "ssl_resnext101_32x8d\n",
      "ssl_resnext101_32x16d\n",
      "swin_base_patch4_window7_224\n",
      "swin_base_patch4_window7_224_in22k\n",
      "swin_base_patch4_window12_384\n",
      "swin_base_patch4_window12_384_in22k\n",
      "swin_large_patch4_window7_224\n",
      "swin_large_patch4_window7_224_in22k\n",
      "swin_large_patch4_window12_384\n",
      "swin_large_patch4_window12_384_in22k\n",
      "swin_small_patch4_window7_224\n",
      "swin_tiny_patch4_window7_224\n",
      "swinnet26t_256\n",
      "swinnet50ts_256\n",
      "swsl_resnet18\n",
      "swsl_resnet50\n",
      "swsl_resnext50_32x4d\n",
      "swsl_resnext101_32x4d\n",
      "swsl_resnext101_32x8d\n",
      "swsl_resnext101_32x16d\n",
      "tf_efficientnet_b0\n",
      "tf_efficientnet_b0_ap\n",
      "tf_efficientnet_b0_ns\n",
      "tf_efficientnet_b1\n",
      "tf_efficientnet_b1_ap\n",
      "tf_efficientnet_b1_ns\n",
      "tf_efficientnet_b2\n",
      "tf_efficientnet_b2_ap\n",
      "tf_efficientnet_b2_ns\n",
      "tf_efficientnet_b3\n",
      "tf_efficientnet_b3_ap\n",
      "tf_efficientnet_b3_ns\n",
      "tf_efficientnet_b4\n",
      "tf_efficientnet_b4_ap\n",
      "tf_efficientnet_b4_ns\n",
      "tf_efficientnet_b5\n",
      "tf_efficientnet_b5_ap\n",
      "tf_efficientnet_b5_ns\n",
      "tf_efficientnet_b6\n",
      "tf_efficientnet_b6_ap\n",
      "tf_efficientnet_b6_ns\n",
      "tf_efficientnet_b7\n",
      "tf_efficientnet_b7_ap\n",
      "tf_efficientnet_b7_ns\n",
      "tf_efficientnet_b8\n",
      "tf_efficientnet_b8_ap\n",
      "tf_efficientnet_cc_b0_4e\n",
      "tf_efficientnet_cc_b0_8e\n",
      "tf_efficientnet_cc_b1_8e\n",
      "tf_efficientnet_el\n",
      "tf_efficientnet_em\n",
      "tf_efficientnet_es\n",
      "tf_efficientnet_l2_ns\n",
      "tf_efficientnet_l2_ns_475\n",
      "tf_efficientnet_lite0\n",
      "tf_efficientnet_lite1\n",
      "tf_efficientnet_lite2\n",
      "tf_efficientnet_lite3\n",
      "tf_efficientnet_lite4\n",
      "tf_efficientnetv2_b0\n",
      "tf_efficientnetv2_b1\n",
      "tf_efficientnetv2_b2\n",
      "tf_efficientnetv2_b3\n",
      "tf_efficientnetv2_l\n",
      "tf_efficientnetv2_l_in21ft1k\n",
      "tf_efficientnetv2_l_in21k\n",
      "tf_efficientnetv2_m\n",
      "tf_efficientnetv2_m_in21ft1k\n",
      "tf_efficientnetv2_m_in21k\n",
      "tf_efficientnetv2_s\n",
      "tf_efficientnetv2_s_in21ft1k\n",
      "tf_efficientnetv2_s_in21k\n",
      "tf_inception_v3\n",
      "tf_mixnet_l\n",
      "tf_mixnet_m\n",
      "tf_mixnet_s\n",
      "tf_mobilenetv3_large_075\n",
      "tf_mobilenetv3_large_100\n",
      "tf_mobilenetv3_large_minimal_100\n",
      "tf_mobilenetv3_small_075\n",
      "tf_mobilenetv3_small_100\n",
      "tf_mobilenetv3_small_minimal_100\n",
      "tnt_b_patch16_224\n",
      "tnt_s_patch16_224\n",
      "tresnet_l\n",
      "tresnet_l_448\n",
      "tresnet_m\n",
      "tresnet_m_448\n",
      "tresnet_m_miil_in21k\n",
      "tresnet_xl\n",
      "tresnet_xl_448\n",
      "tv_densenet121\n",
      "tv_resnet34\n",
      "tv_resnet50\n",
      "tv_resnet101\n",
      "tv_resnet152\n",
      "tv_resnext50_32x4d\n",
      "twins_pcpvt_base\n",
      "twins_pcpvt_large\n",
      "twins_pcpvt_small\n",
      "twins_svt_base\n",
      "twins_svt_large\n",
      "twins_svt_small\n",
      "vgg11\n",
      "vgg11_bn\n",
      "vgg13\n",
      "vgg13_bn\n",
      "vgg16\n",
      "vgg16_bn\n",
      "vgg19\n",
      "vgg19_bn\n",
      "visformer_small\n",
      "visformer_tiny\n",
      "vit_base_patch16_224\n",
      "vit_base_patch16_224_in21k\n",
      "vit_base_patch16_224_miil\n",
      "vit_base_patch16_224_miil_in21k\n",
      "vit_base_patch16_384\n",
      "vit_base_patch32_224\n",
      "vit_base_patch32_224_in21k\n",
      "vit_base_patch32_384\n",
      "vit_base_r26_s32_224\n",
      "vit_base_r50_s16_224\n",
      "vit_base_r50_s16_224_in21k\n",
      "vit_base_r50_s16_384\n",
      "vit_base_resnet26d_224\n",
      "vit_base_resnet50_224_in21k\n",
      "vit_base_resnet50_384\n",
      "vit_base_resnet50d_224\n",
      "vit_huge_patch14_224_in21k\n",
      "vit_large_patch16_224\n",
      "vit_large_patch16_224_in21k\n",
      "vit_large_patch16_384\n",
      "vit_large_patch32_224\n",
      "vit_large_patch32_224_in21k\n",
      "vit_large_patch32_384\n",
      "vit_large_r50_s32_224\n",
      "vit_large_r50_s32_224_in21k\n",
      "vit_large_r50_s32_384\n",
      "vit_small_patch16_224\n",
      "vit_small_patch16_224_in21k\n",
      "vit_small_patch16_384\n",
      "vit_small_patch32_224\n",
      "vit_small_patch32_224_in21k\n",
      "vit_small_patch32_384\n",
      "vit_small_r26_s32_224\n",
      "vit_small_r26_s32_224_in21k\n",
      "vit_small_r26_s32_384\n",
      "vit_small_resnet26d_224\n",
      "vit_small_resnet50d_s16_224\n",
      "vit_tiny_patch16_224\n",
      "vit_tiny_patch16_224_in21k\n",
      "vit_tiny_patch16_384\n",
      "vit_tiny_r_s16_p8_224\n",
      "vit_tiny_r_s16_p8_224_in21k\n",
      "vit_tiny_r_s16_p8_384\n",
      "vovnet39a\n",
      "vovnet57a\n",
      "wide_resnet50_2\n",
      "wide_resnet101_2\n",
      "xception\n",
      "xception41\n",
      "xception65\n",
      "xception71\n"
     ]
    }
   ],
   "source": [
    "for i in timm.list_models():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
